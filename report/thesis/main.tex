\documentclass[12pt,twoside]{report}

% Add the necessary package to define the lstlisting environment
\usepackage{listings}

% Some definitions for the title page
\newcommand{\reporttitle}{Ochre: A Dependently Typed Systems Programming Language}
\newcommand{\reportauthor}{Charlie Lidbury}
\newcommand{\supervisorA}{Steffen van Bakel}
\newcommand{\supervisorB}{Nicolas Wu}
\newcommand{\reporttype}{MEng Individual Project}
\newcommand{\degreetype}{MEng Computing}

% Load some definitions and default packages
\input{includes}

% Load some macros
\input{notation}

% Custom commands
\newcommand{\lochre}{$\lambda_\text{Ochre}$}

% Load title page
\begin{document}
\input{titlepage}


% Page numbering etc.
\pagenumbering{roman}
\clearpage{\pagestyle{empty}\cleardoublepage}
\setcounter{page}{1}
\pagestyle{fancy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
This research presents Ochre, a dependently typed, low-level systems language. In Ochre, programmers can use the type system to prove stronger properties about their programs than they can in non-dependently typed languages such as Rust or Haskell. Ochre also gives programmers low-level enough control over their programs to be able to express efficient in-place algorithms and control the memory layout of user-defined data structures, which makes it a systems language, akin to Rust, C, or C++.

This paper presents the formal semantics of Ochre via \lochre{}, an abstract interpretation over \lochre{}, a concrete interpretation, a proof that the abstract interpretation and the concrete interpretation are consistent, and an implementation of Ochre in the form of an embedding into the Rust programming language.
\end{abstract}

\cleardoublepage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgments}
I would like to thank my supervisor Steffen van Bakel for his type system wisdom, relentless skepticism, and for giving me the freedom to explore such a high-risk project with very little bearing on his research. Steffen even involved his son Isaac van Bakel to help us understand RustBelt and Aeneas, prior work which Ochre takes heavy inspiration from.

I would also like to extend as much gratitude as is physically possible to do via Latex to David Davies, a previous master's student of Steffen who has proven invaluable throughout this project. David has taught me crucial things about dependent types, spent days getting into the nitty gritty of my ideas to make sure I'm on track, and, most importantly, given me the confidence in myself I needed to commit to this project.

% Jamie Willis

% Steffen

Last, but in no means least, I would like to thank my mother Kate Darracott. As well as giving birth to me, which has arguably enabled this project even more than the aforementioned, Mum came up with the brilliant name "Ochre", after being told no more than "the syntax is going to look a little bit like Rust's". Despite not knowing what syntax is, or the significance of dependently typed low-level systems programming languages, she may well have had the most visible contribution to this project of anyone. 

\newpage
\section*{Ethical Considerations}
Much like Wittgenstein, I believe there is an equivalence between ethics and aesthetics; if you do not, here are a few parallels between the two you might find thought-provoking: We do not choose what we deem ethically permissive, much like we do not choose what we find beautiful. Pursuing one's ethical convictions is not a means to an end, it is an end in and of itself, much like aesthetic experiences.

I and many others including cite cite cite, find aesthetic value in problems \& concepts turning out to be reduceable to each other and equivalences being drawn between distant domains. Some particularly high-profile instances of this happening include Euler's formula, the Curry-Howard correspondence and the Church-Turing thesis. To a smaller degree, I also think it happened with Rust's borrow checker, in solving memory management they also solved concurrency, iterator invalidation, and a few other problems that plagued imperative languages.

Despite being sufficiently arrogant and pretentious, I know Ochre isn't as singificant or as beautiful as the previously mentioned identities and isomorphisms. But, in the walled garden of my special interests and obsessions, I have found great aesthetic value in the interplay between ownership semantics and dependent types.

From this aesthetic value, and its equivalence to moral value, I conclude that this research is ethically permissible; I hope Imperial's ethical approval process will too.

\clearpage{\pagestyle{empty}\cleardoublepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%--- table of contents
\fancyhead[RE,LO]{\sffamily {Table of Contents}}
{\small
\begingroup
\setlength{\parskip}{0pt} % No paragraph spacing
\setlength{\parindent}{0pt} % No paragraph indentation
\renewcommand{\baselinestretch}{0.9} % Adjust line spacing
\tableofcontents
\endgroup
}


% \clearpage{\pagestyle{empty}\cleardoublepage}
\pagenumbering{arabic}
\setcounter{page}{1}
\fancyhead[LE,RO]{\slshape \rightmark}
\fancyhead[LO,RE]{\slshape \leftmark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
(TAKEN FROM AENEAS FOR NOW)

In 2006, exasperated by yet another crash of his building's elevator's firmware, and exhausted after walking up 21 flights of stairs, Graydon Hoare set out to design a new programming language \citep{rust-anecdote}. The language, soon to be known as Rust, had two goals. First, to be system-oriented, meaning the programmer would deal with references, pointers, and manually manage memory. Second, to be safe, meaning the compiler's static discipline would rule out memory errors such as use-after-free, or arbitrary memory access. Even though the language evolved a great deal since its inception, these two core premises remain today. 

Eighteen years later, Rust enjoys a substantial amount of success and has ranked as the most loved programming language for 7 consecutive years on StackOverflow's developer survey~\citep{stackoverflow}, until they changed the phrasing of the question in 2023 in which it was the most \textit{admired} language. But as the systems community can attest~\citep{klein2009sel4,lorch2020armada,ferraiuolo2017komodo,bhargavan2017everest}, memory safety is too weak of a property, no matter how remarkable of an achievement Rust is.

We have attempted to prove further properties 

\section{The Problem}
This research hopes to develop a type-checker that is capable of type-checking languages that support both mutation and a kind of type called dependent types. It will do this by removing mutation from the code before type checking, so the type checker only has to reason about immutable code.

Dependent types are covered properly in the background section, but for now, it's enough to know they're a feature that allows you to check even more properties than just type safety at compile time. For instance, instead of just being able to say a variable $x$ is an integer, you can say it's an \textit{even} integer, and reject programs like $x := 5$ at compile time, instead of waiting for them to go wrong at runtime.

This type-checker will support mutation, which is when a variable's value is changed. For instance, when a variable is declared with a value like $x = 2$, then later given a new value like $x := 5$. The most popular languages all support mutation [cite], it's somewhat the (industry) default. Some languages choose to be \textit{immutable} however, which means they do not support mutation. These include Haskell, and almost all languages with dependent types like Agda, Idris, and Coq.

This type-checker is being built to hopefully be used for a larger, more useful language in the future, called Ochre. Ochre which will have both the speed of \textit{systems languages} like C and Rust and the ability to reason about runtime behaviour at compile time of \textit{theorem provers} like Agda and Coq. Exactly what systems languages and theorem provers are is discussed in Chapter \ref{prerequisites}.

For now, I plan on presenting this type-checker in the form of an implementation; however, there is a good argument for focusing more on the theory behind this type-checker, for instance by presenting a set of typing rules or an abstract algorithm. Whether an implementation-heavy or theory-heavy approach is better is an open, and very important question.

\subsection{Why Is It Hard?}
The problem with having these features together in the same language is that a value that another variable's type depends on can be mutated, which changes the \textit{type} of the other variable. Concretely: if we have a variable $x: T$, and another variable $y: F(x)$ whose type depends on $x$, we can assign a new value to $x$ which in turn changes the type $F(x)$; now $y$ is ill-typed because its type has changed, but not it's value. The programmer could fix this by reassigning $y$ with a new value of type $F(x)$, if this happens before $y$ is ever used, the compiler should be able to identify this interaction as type-safe.

\section{The Solution}
\label{thesolution}
The technique this research presents goes as follows: convert the source code from the programmer, which will contain mutation, into a functionally equivalent (but maybe inefficient) immutable version, which can be dependently type-checked. Once this immutable version has been type-checked, the original mutable version can be executed, with full efficiency granted to it by mutability.

Because this translation has been shown to be behaviour preserving\citep{ullrich_electrolysis_nodate} we know properties we prove about the immutable version of the programmer's code also hold for the mutable version which will be executed.

\section{Motivation}
The main contribution of this research will be progress towards making a language that supports both mutability and dependent types, so the motivation behind this research will be the motivation behind these two features, as well as their combination.

This section refers to technical concepts that haven't been explained yet, such as dependent types. The reader is advised to refer to Chapter \ref*{background} if they find concepts being referenced that they do not understand.

\subsection{Mutation}
This section argues why one would want mutation in a programming language.

\subsubsection{Performance}
Some data structures and operations, such as hash maps and their $O(1)$ access/modification, need to modify data in place to be efficiently implemented. Immutable languages like Haskell get around this by performing these mutable operations via unsafe escape hatches and then wrapping those in monads to sequence the immutable operations. However, this often makes mutable code harder to maintain and harder for beginners to understand. For instance, to operate on two hash maps at the same time, you would have to be operating within multiple monads simultaneously, which involves monad transformers or effect types, a much more advanced skillset than what would be required to do the same in Python.

This has widespread effects on the data structures programmers use, and how they structure their programs. Often programmers in immutable languages will simply switch to data structures that don't perform as well but are easier to use in a pure-functional context, like tree-based maps and cons lists instead of hash-maps and vectors.

The performance of explicit mutation can also be easier to reason about. For instance, the Rust code which increments every value in a list of integers doesn't perform any allocations: \verb|for x in xs.iter_mut() { x += 1 }|; whereas the Haskell equivalent looks like it allocates a whole new list, and relies on compiler optimizations to be efficient: \verb|map (+1) xs|. In fact, in this example, Haskell does not do the update in-place and instead allocates a new list in case the old one is being referred to somewhere else. Languages like Koka

\subsubsection{Usability}
Some algorithms are best thought of in terms of mutable operations, and new programmers especially tend to write stuff mutably. By embracing this in the language design, we can come to the user instead of making the user come to us.

Since the CPU is natively works on mutable operations, if you want control over what the CPU does, which you do if you want to extract all the performance you can from it, you want the language to have graceful support for mutation.

\subsubsection{The Immutability Argument}
Proponents of immutability argue immutability helps you reason about your program; since there are no side effects of function calls, you cannot be tripped up by side effects you didn't see coming.

I think this correctly identifies that aliased mutation is bad, but goes too far by removing all mutation. In languages like Rust, only one \textit{mutable} reference can exist to any given memory location, which is needed to write to that memory. This gives you most of the benefits of mutation while avoiding the uncontrolled side effects.

\subsubsection{Popularity}
The majority is often wrong, but it's a good sign if significant proportions of the industry agree on something. In the last quarter of 2023, at least 97.24\% of all committed code was written in a language with mutation [cite: GitHut]. At the very least this shows that people like languages with mutability, even if they are wrong to do so.

\subsection{Dependent Types}
This section argues why one would want dependent types in a programming language.

\subsubsection{Formal Verification}
Dependent types are one of the ways to mechanize logical reasoning, which allows you to reason about the correctness of your programs. For instance, a program that sorts lists should have (amongst other things) the property that it always outputs a list with ascending items. In a language with dependent types, you can make the type of a function express the fact that not only will it return a list of integers, but that it will be a sorted list of integers.

The goal of Ochre, the language this research is done in the name of, is to enable formal verification of low-level systems code. There are other ways to do formal verification, but this is a popular and natural one.

\subsubsection{Usability}
Dependent types are a notoriously difficult feature to learn and reason about, and their ergonomics are underexplored due to them only being used in very niche, academic languages. However, I think if you're not using them for their extra power, they can be just as ergonomic as typical type systems. In this sense, if the language is designed correctly, you only pay for what you use.

\subsection{Mutation + Dependent Types}
This section explains why mutability and dependent types combine to form more than the sum of their parts.

If you use the mutability to make the language high performance, you can use mutability and dependent types to do formal verification of high performance code. This is a common combination of requirements because they both occur when software is extremely widespread and has very high budget.

\subsection{This Particular Method}
This section explains what advantages this particular method has over other combinations of mutability and dependent types, such as ATS, Magmide, and Low*.

This type checker allows the types and mutable values to be unusually close. In ATS for instance there are basically two separate languages: a dependently typed compile time language and a mutable run-time language. This creates lots of overhead manually linking the two together. For instance, $x : int(y)$ means an integer $x$ with value $y$. In compile time contexts, you use $y$ to refer to the value, in runtime contexts you use $x$. I hope to remove the need for this distinction.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Background}
\label{background}

\section{Dependent Types}

\section{Rust}
Rust is a modern programming language that offers a unique combination of strong (memory) safety guarantees and bare-metal performance. Rust innovates in other areas relevant to software engineering, but for this research performance and safety are the two key features which will be built upon.

\subsubsection{Performance}
Rust is a fast language. Its performance is roughly equivalent to that of C and C++ \cite{RustVsWhich}, which are generally accepted as the benchmark of language performance. Rust has enduring performance problems \cite{AreWeStackefficient2022}, but it is fair to say that on the whole there aren't major performance differences between the fastest languages. The fastest programming languages have more or less hit a ceiling of performance, with no major improvements in speed even since Fortran \cite{GccVsClassic} which dates back to 1957 \cite[p. 16]{wilsonComparativeProgrammingLanguages2001}.

Making a fast programming language is more about removing slow features than it is about introducing ones that explicitly help performance. Languages like Haskell and Java automatically handle memory allocation and deallocation at the cost of having to have a garbage collector that periodically scans the heap and deallocates inaccessible objects; this is an example of a feature that reduces performance.

Rust is a fast language because it doesn't have a runtime or garbage collector, and has an efficient memory layout. In languages like Haskell or Java, almost all data is heap-allocated and deallocated automatically via a 

To generate optimal code, systems languages let the programmer manage their memory, and choose memory layouts. In doing so, they typically sacrifice the memory safety guarantees higher-level languages make due to not being able to check the programmer has managed their memory correctly, this is the case in C and C++. Rust uses a concept called \textit{ownership} to recover these memory safety guarantees while still giving the programmer sufficient control to match C and C++'s performance.

\subsubsection{Ownership}
Ownership is a set of rules that govern how a Rust program manages memory. All programs have to manage the way they use a computer’s memory while running. Some languages have garbage collection that regularly looks for no-longer-used memory as the program runs; in other languages, the programmer must explicitly allocate and free the memory. Rust uses a third approach: memory is managed through a system of ownership with a set of rules that the compiler checks. If any of the rules are violated, the program won’t compile. None of the features of ownership will slow down your program while it’s running. \footnote{Paragraph taken from the Rust Book https://doc.rust-lang.org/book/ch04-01-what-is-ownership.html which I highly recommend for a deeper explanation of ownership.}

There are three rules associated with ownership in Rust:
\begin{itemize}
  \item Each value in Rust has an owner.
  \item There can only be one owner at a time.
  \item When the owner goes out of scope, the value will be dropped.
\end{itemize}

\subsubsection{Borrowing And The Borrow Checker}
A consequence of only being able to have one owner of any given value at a time is that passing a value to a function invalidates the variable that used to hold that value. This is referred to as the ownership \textit{moving}. For instance:

\begin{lstlisting}
  let x = Box::new(5);
  f(x); // Ownership of x passed to f
  g(x); // Invalid, we no longer have ownership of x
\end{lstlisting}

To get around this we could get the functions to give ownership back to us when they return, but this is very syntax-heavy. Rust uses a concept called borrowing in this scenario, which allows you to temporarily give a function access to a value, without giving it ownership. The above example would be done like so:

\begin{lstlisting}
  let x = Box::new(5);
  f(&x);
  g(&x); // Now works
\end{lstlisting}

Here, \verb|&x| denotes a \textit{reference} to \verb|x|. At runtime, this is represented as a pointer. There are two different types of references in Rust: immutable references, denoted by \verb|&T|, and mutable references denoted by \verb|&mut T|. For any given value, you can either hold a single mutable reference or $n$ immutable references, but never both at the same time. This is called the aliasing xor (exclusive or) constraint, or AXM for short.

The borrow checker keeps track of when these references exist to ensure AXM is being upheld. To do this the programmer must annotate references with lifetime annotations, so the compiler has the information of how long the programmer intends each reference to last. Checking these lifetimes overlap in compatible ways is the job of the borrow checker.


\section{Aeneas \& The LLBC}

\section{Prerequisite Concepts}
\label{prerequisites}
This section explains the concepts required to understand this research.

\subsection{Mutability}
Mutability is when the value of a variable can change at runtime. For instance in Rust, \verb|let mut x = 5; x = 6;| first assigns the value $5$ to the variable $x$, then updates it to $6$, which means the value of $x$ depends on the point within the programs execution. This becomes more relevant when you have large objects that get passed around your program, like \verb|let mut v = Vec::new(); v.push(1); v.push(2);| which makes a resizable array on the heap, then pushes $1$ and $2$ to it.

In Rust to make a variable mutable you must annotate its definition with \verb|mut|, but in most languages, it is just always enabled, like in C \verb|int x = 5; x = 6;| works.

\subsection{Dependent Types}
A dependent type is a type that can change based on the value of another variable in the program. For instance, you might have a variable $y$ which is sometimes an integer, and sometimes a boolean, depending on the value of another variable, $x$.

When discussing dependent types, there are two important dependent type constructors: $\Sigma$ and $\Pi$. They're usually referenced together because they're roughly equivalent; the dual of $\Sigma$ types are $\Pi$ types and visa versa, which apparently means something to category theorists. In the following, I use $Vec(\mathbb{Z}, n)$ to denote the type of an $n$-tuple of integers, i.e. $(1, 2, 3): Vec(\mathbb{Z}, 3)$.

\begin{itemize}
  \item \textbf{Dependent Functions} ($\Pi$ Types) - A dependent function is one whose return type depends on the input value. For instance, you could define a function $f$ which takes a natural $n$, and returns $n$ copies of $42$ in a tuple i.e. $f(3) = (42, 42, 42)$. $f$'s type would be denoted as $f: (\textbf{n}: \mathbb{N}) \rightarrow Vec(\mathbb{Z}, \textbf{n})$ in Agda/Ochre syntax, or $f: \Pi_{\textbf{n}: \mathbb{N}} Vec(\mathbb{Z}, \textbf{n})$ in a more formal mathematical context.
  \item \textbf{Dependent Pairs} ($\Sigma$ Types) - A dependent pair is a pair where the type of the right element depends on the value of the left element. For instance, you could define a pair $p$ which holds a natural $n$ and a $n$-tuple of integers i.e. $p = (3, (42, 42, 42))$. $p$'s type would be denoted as $p: (\textbf{n}: \mathbb{N}, Vec(\mathbb{Z}, \textbf{n}))$ in Agda/Ochre syntax, or $p: \Sigma_{\textbf{n}: \mathbb{N}} Vec(\mathbb{Z}, \textbf{n})$ in a more formal mathematical context.
\end{itemize}

A language supports dependent types if it can type-check objects like the aforementioned $f$ and $s$. Just allowing them to exist is not enough. For instance, Python is not dependently typed just because a function's return type can depend on its input, because its type checker doesn't reject programs when you do this wrong. $f$ can be typed in Agda, a dependently typed language with $f: (n: \mathbb{N}) \rightarrow Vec(\mathbb{Z}, n)$ but has no valid type in Haskell, which doesn't support dependent types.

\subsection{Formal Verification with Dependent Types}
While dependent types can be nice to have by themselves, a large part of their motivation is using them to perform formal verification.

\textbf{If you are willing to accept that dependent types can be used to perform formal verification, you do not need to understand how dependent types can be used for logical reasoning}: none of this information will be used since the goal of this research is not to perform formal verification, it's just to do dependent type checking.

Readers who are nonetheless interested are invited to read Appendix \ref{verificationwithtypes}.

\subsection{Rust}
The mutable $\rightarrow$ immutable translation this research relies on requires lifetime annotations to work. While ownership and lifetimes are standalone concepts, their only real-world use case so far has been memory management in the Rust programming language. This section explains these concepts in the context of Rust.

% cut out rust bits

\subsection{Mutable $\rightarrow$ Immutable Translation}
To reason about and type-check the mutable code from the programmer, the type checker this research presents translates the source code into an immutable version, as outlined in Section \ref{thesolution}.

The crux of this translation is the observation that \textbf{a function that mutates a value can be replaced by one that instead returns the new value}. I.e. if the programmer writes a function with type \verb|&mut i32 -> ()|, it can be replaced by \verb|i32 -> i32|. Which would then be used like this:

\noindent\begin{minipage}{.45\textwidth}
\begin{lstlisting}[caption=Original]{Name}
let mut x = 5;
f(&mut x); // Mutates x
\end{lstlisting}
\end{minipage}\hfill
\begin{minipage}{.45\textwidth}
\begin{lstlisting}[caption=Translated]{Name}
let x = 5;
let x = f(x); // Re-defines x
\end{lstlisting}
\end{minipage}

The complexity of this translation comes in handling all language constructs in the general case, for instance, if statements need to return the values they edit. Like so:

\noindent\begin{minipage}{.45\textwidth}
\begin{lstlisting}[caption=Original]{Name}
let mut x = 5;
if x > 3 {
  x = x + 1; // Mutation
}
\end{lstlisting}
\end{minipage}\hfill
\begin{minipage}{.45\textwidth}
\begin{lstlisting}[caption=Translated]{Name}
let x = 5;
let x = if x > 3 {
  x + 1
} else {
  x
};
\end{lstlisting}
\end{minipage}

This quickly gets complicated when you start to use more advanced features like for loops and functions which return mutable references \footnote{See \citep{aeneas} Chapter 2 \textit{Aeneas and its Functional Translation, by Example} for explanation of returning mutable references. (Search for ``Returning a Mutable Borrow, and a Backward Function'') for the exact paragraph.}. So much so safe Rust isn't even entirely covered by the two main attempts at this translation Electrolysis \citep{ullrich_khaelectrolysis_2024} and Aeneas \citep{aeneas} \footnote{See Figure 14 of \citep{aeneas} for a table showing roughly which features are covered by Aeneas/Electrolysis, and see https://kha.github.io/electrolysis/ for exact Rust coverage for Electrolysis.}. In this research I don't intend to support any constructs not already supported by either of these prior works, so I can use the translation algorithms they have already developed.

\section{Related Work}
Related work comes under two main categories: research which works towards combining mutability with dependent types, and more general work which works towards formal verification of low level code.

\subsection{Languages with Mutability and Dependent Types}

\subsubsection{ATS}
ATS \citep{xi_applied_2017} is the most mature systems programming language to date, with work dating back to 2002 \citep{ATSImplements}. As its website states, it is a \textit{statically typed programming language that unifies implementation with formal specification} \citep{ATSHome}.

It's more or less an eagerly evaluated functional language like OCaml, but with functions in the standard library that manipulate pointers, like \verb|ptr_get0| and \verb|ptr_set0| which read and write from the heap respectively. To read or write to a location in memory, you must have a token that represents your ownership of the memory, called a \textit{view}.

For instance, the \verb|ptr_get0| function has the type $\{l:addr\} (T @ l | ptr (l)) \rightarrow (T @ l | T)$ where
\begin{itemize}
  \item $\{l:addr\}$ means for all memory addresses, $l$
  \item $|$ is the pair type constructor
  \item $T @ l$ means ownership of a value of type $T$, at location $l$. Since it is both an input and an output, this function is only \textit{borrowing} ownership.
  \item $ptr(l)$ means a pointer pointing to location $l$. Since it can only point at location $l$, it is a singleton type. This is used to convert the static compile-time variable $l$ into an assertion about the runtime argument.
\end{itemize}


So overall, this type reads ``for all memory addresses $l$, the function borrows ownership of location $l$, and turns a pointer to location $l$ into a value of type $T$''.

This necessity to manually pass ownership around introduces a lot of administrative overhead to ATS, which is one of the reasons it is a notoriously hard language to learn/use. ATS introduces syntactic shorthand for these things which you can use in simple cases to clean things up, but still requires this proof passing in many cases which would be dealt with automatically by Rust's borrow checker.

Over the years several versions of ATS have been built, with interesting differences in approach. The current version, ATS2 has only a dependent type-checker, whereas the in-progress ATS3 uses both a conventional ML-like type-checker, as well as a dependent type-checker, and approach that the author of ATS himself developed in separate research, from which ATS3 gets its full name, ATS/Xanadu.

\subsubsection{Magmide}
The goal of Magmide \citep{noauthor_magmidemagmide_2024} is to ``create a programming language capable of making formal verification and provably correct software practical and mainstream''. Currently, Magmide is unimplemented, and there are barely even code snippets of it. However, there is extensive design documentation in which the author Blaine Hansen lays out the compiler architecture he intends to use, which involves two internal representations: \textit{logical} Magmide and \textit{host} Magmide.

\begin{itemize}
  \item Logical Magmide is a dependently typed lambda calculus of constructions, where to-be-erased types and proofs are constructed.
  \item Host Magmide is the imperative language that runs on real machines. (Hansen intends on using Rust for this)
\end{itemize}

I believe this will mean there are two separate languages co-existing on the front end, much like the separation between type-level objects and value-level objects in a language like Haskell.

I suspect this will cause a similar situation to what you see in ATS where for each variable you care about you have two versions, a compile-time one and a runtime one, but it's hard to tell because of the lack of code examples.

\subsubsection{Low*}
Low*\citep{protzenko_low_2017} is a subset of another language, F*, which can be extracted into C via a transpiler called KreMLin. It has achieved impressive results, mostly at Microsoft Research, where they have used it to implement a formally verified library of modern cryptographic algorithms\citep{star_2024} and EverParse

Its set of allowed features is carefully chosen to make this translation possible in the general case, which restricts the ergonomics of the language, it does not support closures, and therefore higher-order programming for example.

It is very much not a pay-for-what-you-use language, to compile anything you must manually manage things like pushing and popping frames on and off the stack, so even if it can achieve impressive results, it's only useful for teams willing to pay the high price which comes with verifying the entire program. This research aims to be better by not requiring any effort from the programmer in the case that they do not wish to use dependent types for their reasoning power.

\subsection{Embedding Mutability in Languages With Dependent Types}

\subsubsection{Ynot: Dependent Types for Imperative Programs}
Ynot\citep{nanevski_ynot_2008} is an extension of the Coq proof assistant which allows writing, reasoning about, and extracting higher-order, dependently-typed programs with side-effects including mutation. It does so by defining a monad \verb|ST p A q| which performs an effectful operation, with precondition \verb|p|, postcondition \verb|q| and producing a value of type \verb|A|. They also define another monad, \verb|STSep p A q| which is the same as \verb|ST| except it satisfies the frame rule from separation logic: any part of the heap that isn't referenced by the precondition won't be affected by the computation. This means if you prove properties about a \verb|STSep| computation locally, those proofs still apply even when the computation is put into a different context: this is called compositional reasoning. The Ynot paper presents a formally verified mutable hash table.

Ynot is important foundational work in this area which seems to have inspired many of the other related work here, but is itself not up to the task of verifying low-level code for two reasons:

\begin{enumerate}
  \item It cannot be used to create performant imperative programs because all mutation occurs through a Coq monad which limits the performance to what you can do in Coq, which is a relatively slow language. This is in contrast to Low*\citep{protzenko_low_2017} for example which is extracted to C, and therefore unrestricted when it comes to performance.
  \item To do any verification at all, you must use heap assertions, instead of reasoning about the values directly. This is sometimes needed, like when you're doing aliased mutation (verifying unsafe Rust), but usually not; Aeneas\citep{aeneas} claims to be hugely more productive than its competitors by not requiring heap assertions for safe Rust code.
\end{enumerate}

\subsection{Formal Verification of Low-Level Code}
Low-level code, such as C code can be directly reasoned about by theorem provers like Isabelle, as was done to verify an entire operating system kernel SeL4\citep{klein_sel4_2009}. However, going via C like this has major drawbacks: since the source language is very unsafe, you have a lot of proof obligations. For instance, when reasoning about C you must often prove that a set of pointers do not point to the same location, otherwise mutating the value of one might mutate the others. With Rust references you do not need to do this because the type system prevents you from creating aliased pointers.

\subsubsection{Rust Belt}
RustBelt\citep{jung_rustbelt_2018} is a formal model of Rust, including unsafe Rust. Its primary implementation is a Coq framework, Iris\citep{noauthor_iris_nodate} which allows you to model unsafe Rust code in Coq, and prove it upholds Rust's correctness properties.

I see RustBelt as a great complement to this work in the future: real programs require unsafe code, but you want to avoid having to model your code in a separate proof assistant as little as possible. In Ochre, I imagine the few people who write unsafe code will verify it with something like RustBelt, while the majority won't have to, but will benefit from the guarantees provided by the verified libraries they use which do.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Ochre}
\label{section:ochre}
This chapter introduces the various language constructs of Ochre, at first via intuitive examples, then formally. Each language construct's runtime behavior is discussed, then how it is reasoned about statically, which splits this chapter into 4 sections with the following distinctions:

\begin{center}
  \begin{tabular}{c|cc}
    & Runtime Semantics & Static Analysis \\
    \hline
    \\
    Intuition Building & \underline{Section \ref{section:ochreexample}} & \underline{Section \ref{section:checkingexample}} \\
    & Ochre, by Example & Type \& Borrow Checking, by Example \\
    \\
    Formal & \underline{Section \ref{section:concreteinterpretation}} & \underline{Section \ref{section:abstractinterpretation}} \\
    & Concrete Interpretation & Abstract Interpretation \\
  \end{tabular}
\end{center}

The motivations behind Ochre, alternative design decisions, evaluation, implementation, or reasoning about any properties are all explicit non-goals of this Chapter.

\subsubsection{Attribution}
With the exception of references, the primitive types in Ochre are from the $\Pi\Sigma$ language presented in \cite{altenkirch2010pisigma}, including the representation of algebraic data types.

The mutation \& memory management techniques presented are from Rust, including move semantics, references, and the restrictions placed on references.

The novel work presented is the combination of these two features, which requires introducing a new kind of subtyping in which every term is its own type. TypeScript partly does this with literal types, producing results like \mono{5:5}, but this research takes this to its logical conclusion where even functions are their own type, and there is almost no distinction between types and terms apart from the requirement that all types are resolved at compile time.

\todo[inline]{explain double page + hyper links thing}
\todo[inline]{do hyperlinks thing}

\cleardoublepage
\section{Ochre, by Example}
\label{section:ochreexample}
This section covers Ochre in a gradual, example-heavy manner, much like programming language tutorials like The Rust Book \citep{RustProgrammingLanguagea}. The goal of this section is to build an intuition behind the behavior which the type-checking will later reason about.

Ochre is an impure functional language, composed of expressions that can have side effects.

\subsubsection{Basic Language Constructs}
The simplest Ochre value is an \textit{atom}. Atoms are constructed with \mono{'}, for example: \mono{'hello} or \mono{'world}\footnote{The runtime representation of an atom is assumed to be the hash of the string after the tick, which makes them constant length. This allows them to be stack-allocated instead of heap-allocated}. Atoms are an unopinionated primitive type upon which more complex structures can be built.

\begin{minted}{rust}
  'hello
\end{minted}

$M \mono{=} N$ writes the result of evaluating $N$ to $M$, for example: \mono{x='one} sets \mono{x} to $\atom{one}$. Declarations are implicit in Ochre (for now); if \mono{x} was in scope previously, \mono{x='one} will bring it into scope, and if it was already in scope, it will mutate it. $M \mono{;} N$ sequences $M$, then $N$. Line comments are opened with \mono{//}.

\begin{minted}{rust}
  x = 'hello;
  x // 'hello
\end{minted}

\subsubsection{References \& Mutation}
Variables are either modified directly or via a mutable reference. The latter is constructed with \mono{\&mut} and eliminated (dereferenced) with \mono{*}.

\begin{listing}[H]
  \begin{minted}{rust}
    x = 'one;
    x = 'two; // mutates x directly
    rx = &mut x;
    *rx = 'three; // mutates x via a mutable reference
    x // 'three
  \end{minted}
  \caption{Mutation}
  \label{lst:mutref}
\end{listing}

Whilst a mutable reference to a value exists, that value cannot be read or modified directly, it can only be read or modified via the mutable reference. In Listing \ref{lst:mutref}, the use of \mono{x} on line 5 is not an error despite \mono{rx} existing because is implicitly \textit{dropped} just before the usage of \mono{x}. Because of this implicit drop, \mono{rx} cannot be used after line 5.

In practice, this is intolerably restrictive because it means only one pointer can exist to any value at a time. Like Rust, Ochre solves this by supporting \textit{immutable} references, constructed with \mono{\&} and dereferenced with \mono{*}. These allow the programmer to have multiple references to the same value, called \textit{aliasing}. There is a tradeoff that you cannot mutate the referenced value, known as \textit{aliasing xor mutability} (AXM), and it's crucial to how Rust can be converted to pure functional code, or dependently type-checked \citep{aeneas,ullrichKhaElectrolysis2024}.

\begin{listing}[H]
  \begin{minted}{rust}
    x = 'one;
    rx1 = &x;
    rx2 = &x;
    x; // 'one
    *rx1; // 'one
    *rx2; // 'one
  \end{minted}
  \caption{The value \mono{'one} can be accessed via \mono{x}, \mono{rx1}, and \mono{rx2} simultaneously}
  \label{lst:immref}
\end{listing}

\subsubsection{Pairs}
$M \mono{,} \, N$ constructs the pair of $M$ and $N$. Pairs are typically surrounded in brackets to make the precedence explicit. $M\mono{.0}$ and $M\mono{.1}$ access the right and left elements of the pair $M$.

% \begin{listing}[H]
  \begin{minted}{rust}
    x = ('one, 'two);
    x.0; // 'one
    x.1; // 'two
  \end{minted}
% \end{listing}

\subsubsection{Move Semantics}
Ochre uses Rust's ownership semantics to handle manual memory management. Using a value \textit{moves} it, which means it is no longer accessible in the original location. This means you have exclusive access to any value not accessed via an immutable reference. This enables the "whenever a variable goes out of scope, free its associated memory" rule, which is how Rust and Ochre avoid the need for a garbage collector.

Move semantics can lead to some strange results, such as the following program being invalid:

\begin{minted}{rust}
  x = 'one;
  y = x;
  x; // error! use of moved value
\end{minted}

\mono{y = x} moved the value $\atom{one}$ from \mono{x} into \mono{y}, which uninitializes \mono{x}. Moving is granular; you can move components of a pair out of the pair without invalidating the whole pair:

\begin{minted}{rust}
  x = ('unmoved, 'moved);
  y = x.1; // move right component into y
  x.0; // 'unmoved
  x.1; // error! use of moved value
\end{minted}

\subsubsection{Structural Typing and Type Union}
Ochre uses a structural type system. This means a type is entirely defined by the (potentially infinite) set of its inhabitants. This is in contrast to \textit{nominal} typing, where type equivalence depends on the type's name or place of declaration. Take the following type definitions in Rust:

\begin{minted}{rust}
  struct Foo(i32, i32);
  struct Bar(i32, i32);
\end{minted}

\noindent
Both \mono{Foo} and \mono{Bar} are types that can be constructed with a pair of integers\footnote{In Rust, \mono{i32} is the type of 32-bit signed integers.}. In Rust, it would be a type error to pass a \mono{Foo} to a function that expects a \mono{Bar}, because despite holding the same data, they are different types. The equivalent Ochre code would be:

\begin{minted}{rust}
  Foo = (Int, Int);
  Bar = (Int, Int);
\end{minted}

Unlike in nominally typed languages, an Ochre function which expects a value of type \mono{Foo} as input, can be given a value of type \mono{Bar}. Every identifier you use to refer to a type in Ochre is roughly equivalent to a type \textit{alias} in nominally typed languages like Rust and Haskell.

In Ochre, every value is its own type. So \mono{'one} is of type \mono{'one}, which is expressed in Ochre via colon. Non-singleton types are made up by taking the union of other types, using the \mono{|} operator, like \mono{'a | 'b | 'c}, which can be any of \mono{'a}, \mono{'b}, or \mono{'c}.

% \begin{listing}[H]
  \begin{minted}{rust}
    'a: 'a; // valid
    'a: 'a | 'b; // also valid
    'c: 'a | 'b; // type error
  \end{minted}
% \end{listing}

The same goes for references, pairs, and functions (which will be introduced later): the type of a reference is itself a reference, the type of a pair is itself a pair, and the type of a function is itself a function. The only consistent difference between types and terms is types must be statically known, which means they can be erased by runtime.

% \begin{listing}[H]
  \begin{minted}{rust}
    ('a, 'b): ('a, 'b); // valid
    ('a, 'b): ('a | 'b, 'a | 'b); // also valid
  \end{minted}
% \end{listing}

The \mono{*} syntax denotes the infinite type/top, the type that contains all values. This is used to represent the concept of no typing information being available. There are three main places where this comes up:

\begin{enumerate}
  \item Taking the union of two types which don't have a meaningful union, like pairs and atoms. \mono{'a | ('a, 'a): *}.
  \item Using it to represent the type of types, which is how you do generic functions. Polymorphic functions are defined by making a function which takes a type as input, and returns a function which uses that type.
  \item The type of uninitialised/moved data.
\end{enumerate}

\subsubsection{Comptime vs Runtime}

Types, just like values, can be assigned to variables for future re-use. However, they must all be statically known, which is enforced by only allowing them to be assigned to \textit{comptime} variables, which start with capital letters. This is similar to how in Haskell types must start with a capital letter, but here the line between types and values is blurred significantly.

% \begin{listing}[H]
  \begin{minted}{rust}
    abPair = ('a | 'b, 'a | 'b); // error! type union can only occur at compile time
    ABPair = ('a | 'b, 'a | 'b); // valid
    ('a, 'b): ABPair;
  \end{minted}
% \end{listing}

\subsubsection{Functions}
Functions are defined with an arrow \mono{->} and an optional runtime body surrounded in curly braces. For instance, the identity function over \mono{'true | 'false} is defined as such:

% \begin{listing}[H]
  \begin{minted}{rust}
    Bool = 'true | 'false;
    id = (x: Bool) -> Bool { x };
  \end{minted}
% \end{listing}

If the runtime body is omitted, the function can only be called at compile time, which means it must be written to a comp time variable:

\begin{listing}[H]
  \begin{minted}{rust}
    Bool = 'true | 'false;
    Id = (x: Bool) -> Bool; // valid
    id = (x: Bool) -> Bool; // invalid: attempt to assign comptime func to runtime var
  \end{minted}
\end{listing}

The only difference between a function body and its return type is that its return type is run at compile time, there is no syntactic difference. For functions you want to run at compile time, syntax after the arrow \textit{is} the function body.

% \begin{listing}[H]
  \begin{minted}{rust}
    Id = x -> x; // Definition of identity which can only be run at comp time
    id = x -> x { x }; // Definition of identity which also exists at runtime
  \end{minted}
% \end{listing}

\subsubsection{Case Statements}
In Ochre, atoms can be branched on via a case statement. The discriminant of the case statement must be an atom, and there must be exactly one branch for each possible atom. In the future, I plan on adding if and match statements, which will be syntactic sugar for case statements.

% \begin{listing}[H]
  \begin{minted}{rust}
    Bool = 'true | 'false;
    not = (b: Bool) -> Bool {
      case b {
        'true => 'false,
        'false => 'true,
      }
    };
    not('true); // 'false
  \end{minted}
% \end{listing}

\subsubsection{Dependent Pairs}
If a pair is being evaluated in a comptime context, the right of a pair can depend on the left. This is done by making the right a function that maps from left to right.

% \begin{listing}[H]
  \begin{minted}{rust}
    Same = (Bool, L -> L); // binds LHS to L, so can be used by right
    ('true, 'true): Same; // valid
    ('true, 'false): Same; // error! 'false is not of type 'true

    Different = (Bool, L -> case L { 'true => 'false, 'false => 'true});
    ('true, 'false): Different; // valid
    ('true, 'true): Different; // error!
  \end{minted}
% \end{listing}

When you union together pairs, it doesn't just union together their left and right and make a new pair, it uses any information it can get from the left pair to more precisely type the right pair.

\begin{listing}[H]
  \begin{minted}{rust}
    Same = ('true, 'true) | ('false, 'false);
    // Expanded internally to:
    Same = ('true | 'false, L -> case L { 'true => 'true, 'false => 'false })
  \end{minted}
  \vspace{-0.5em}
  \caption{}
  \label{lst:same}
\end{listing}
\vspace{-1em}

This makes the union operator precise, taking the union of two types should never produce a type with inhabitants that weren't in either of the types which were unioned together.

If you want to record dependence between the left and right of a pair in a runtime context, you must construct the pair without the dependence, and then use a type constraint to add it back in.

\begin{listing}[H]
  \begin{minted}{rust}
    Same = ('true, 'true) | ('false, 'false);
    x = ('true, 'true); // x is a non-dependent pair
    x: Same; // type constraint has made x a dependent pair
  \end{minted}
\end{listing}

\subsubsection{Type Narrowing}
If the right of a pair depends on the left, and then you find something out about the left, you should in turn find something out about the right. This is done in Ochre via type \textit{narrowing}. In the below example, we define a function \mono{f}, and within \mono{f} we know that the left and right of our pair \mono{p} are the same (using the definition in Listing \ref{lst:same}). When we match on its left with \mono{p.0}, each branch is type-checked with the additional knowledge that we are in that particular branch. This allows the compiler to correctly identify that when matching on the other side of the pair, you only need to have one branch.

\begin{listing}[H]
  \begin{minted}{rust}
    Same = ('true, 'true) | ('false, 'false);
    f = (p: Same) -> Bool {
      case p.0 {
        'true => case p.1 { 'true => 'unit }, // p.1: 'true
        'false => case p.1 { 'false => 'unit }, // p.1: 'false
      }
    }
  \end{minted}
  \caption{Case statements narrow down the type of their discriminant in each branch}
\end{listing}

\subsubsection{Algebraic Data Types}
Take the following definition of Peano naturals in Haskell syntax:

% \begin{listing}[H]
  \begin{minted}{haskell}
    data Nat = Zero | Succ Nat
  \end{minted}
% \end{listing}

In Ochre this is represented by a dependent pair. The left of the pair indicates which variant the ADT is in (either zero or successor), and the right contains the payload of that variant. In the zero case, nothing is stored, so the payload is \mono{'unit}, in the successor case, we store the natural that we are the successor of, so our payload is \mono{Nat}.

% \begin{listing}[H]
  \begin{minted}{rust}
    // "manual" ADT encoding
    Nat = (T: 'zero | 'succ, case T { 'zero => 'unit, 'succ => Nat });
    // idiomatic encoding using type union
    Nat = ('zero, 'unit) | ('succ, Nat);
  \end{minted}
% \end{listing}

By matching on the left, you can determine which variant the ADT is in, then you can access the payload through the right. For instance, this is how would define addition over Peano naturals:

% \begin{listing}[H]
  \begin{minted}{rust}
    Nat = ('zero, 'unit) | ('succ, Nat);
    add = (x: Nat, y: Nat) -> Nat {
      case x.0 {
        'zero => y, // 0 + y = y
        'succ => ('succ, add(x.1, y)), // (1 + x) + y = 1 + (x + y)
      }
    }
  \end{minted}
% \end{listing}

\subsubsection{Recursion}
The definition of \mono{add} above won't compile because of how it does recursion. When type-checking assignments, Ochre looks at the left first to figure out what type the identifiers have. In the case of \mono{Nat} and \mono{add} above there are no type annotations, so it evaluates the assigned value with no extra type information.

If the programmer puts type annotations on the left of an assignment, the compiler knows at least something about the type, so it can evaluate the expression with that knowledge. This isn't required in the definition of \mono{Nat} because you can put anything in a pair, regardless of its type, so the usage of \mono{Nat} on the right was permissible.

In the \mono{add} case, we need to know that \mono{add} has type {(Nat, Nat) -> Nat} while evaluating the function body, so we can check that \mono{add(x.1, y)} has type \mono{Nat}. To introduce this, add type annotations to the left of the assignment:

% \begin{listing}[H]
  \begin{minted}{rust}
    add: (Nat, Nat) -> Nat = (x: Nat, y: Nat) -> Nat {
      // ...
    }
  \end{minted}
% \end{listing}

This introduces repetition in the types, which we remove by adding the following syntactic sugar for the above:

% \begin{listing}[H]
  \begin{minted}{rust}
    add(x: Nat, y: Nat): Nat = {
      // ...
    }
  \end{minted}
% \end{listing}

\cleardoublepage
\section{Type \& Borrow Checking, by Example}
\label{section:checkingexample}
This section aims to give the reader an intuition behind the abstract interpretation used to type-check Ochre. Specifically, it answers two questions: what is the abstract environment? And how do the various syntactic constructs modify it?

The abstract environment is a mapping from identifiers to types, although it can often look like a mapping from identifiers to values because the type of a value like \mono{'true} is $'true$. It stores the types of both runtime and comptime variables, which are distinguished by comptime variables starting with a capital letter.

Throughout this thesis, syntax will be in monospace font \mono{like this}, and abstract values will be in mathematical text $like\,this$.

\subsubsection{Basic Language Features}
Type-checking an Ochre program always starts with an empty environment, and every time information is gained, it is added to the abstract environment. Like so. The type of every atom $\mono{'a}$ is the singleton set $\{'a\}$, but it is also every superset of that singleton set like $\{'a, 'b\}$.

\begin{listing}[H]
  \begin{minted}[mathescape]{rust}
    x = 'true; // $\{ \absmap{x}{\{\atom{true}\}} \}$
    y = 'hello; // $\{ \absmap{x}{\{\atom{true}\}}, \absmap{y}{\{\atom{hello}\}} \}$
    x = 'false; // $\{ \absmap{x}{\{\atom{false}\}}, \absmap{y}{\{\atom{hello}\}} \}$
  \end{minted}
  \caption{A series of assignments, and their corresponding effects on the abstract environment.}
  \label{lst:atomabstract}
\end{listing}

In the above example, it would be sound for the abstract environment to map $x$ onto $\{'true, 'false\}$, or even $\{'true, 'unrelated\}$, but that would be losing information. The concept of losing typing information will be made explicit later with environment \textit{rearrangements}, but for now, we'll focus on the environment being as precise as possible.

For brevity, we use $'a$ as syntactic sugar for the singleton set $\{'a\}$. This never causes ambiguity because the abstract environment only ever uses atoms in sets, never by themselves.

\begin{listing}[H]
  \begin{minted}[mathescape]{rust}
    x = 'true; // $\{ \absmap{x}{\atom{true}} \}$
    y = 'hello; // $\{ \absmap{x}{\atom{true}}, \absmap{y}{\atom{hello}} \}$
    x = 'false; // $\{ \absmap{x}{\atom{false}}, \absmap{y}{\atom{hello}} \}$
  \end{minted}
  \caption{Listing \ref{lst:atomabstract} but using syntactic sugar for singlton sets of atoms.}
\end{listing}

When you move a value, it is mapped to $\bot$ in the abstract environment:

\begin{minted}[mathescape]{rust}
  x = 'hello; // $\{ \absmap{x}{\atom{hello}} \}$
  y = x; // $\{ \absmap{x}{\bot}, \absmap{y}{\atom{hello}} \}$
\end{minted}

\subsubsection{References \& Mutation}

When you construct a reference, the value is \textit{borrowed}. In the case of mutable borrows, this means the value isn't available in the original location, which is represented in the abstract environment as $\kw{loan}^m \, l$ where $l$ is the \textit{loan identifier} for this particular loan. We set it to this instead of $\bot$ so we can find it again in the future when we want to terminate the loan. The reference will map to $\borrowm{l}{v}$ where $v$ is the type of the value being borrowed.

\begin{listing}[H]
  \begin{minted}[mathescape]{rust}
    x = 'one; // $\{ \absmap{x}{\atom{one}} \}$
    rx = &mut x; // $\{ \absmap{x}{\loanm{l}}, \absmap{rx}{\borrowm{l}{\atom{one}}} \}$
    *rx = 'two; // $\{ \absmap{x}{\loanm{l}}, \absmap{rx}{\borrowm{l}{\atom{two}}} \}$
    // rx dropped
    x; // $\{ \absmap{x}{\atom{two}}, \absmap{rx}{\bot} \}$
  \end{minted}
  \caption{A reference to a variable being constructed and used for a mutation. When the reference \mono{rx} is dropped, the updated value from the mutable reference is written back to the original variable \mono{x}.}
\end{listing}

\begin{listing}[H]
  \begin{minted}[mathescape]{rust}
    x = 'one; // $\{ \absmap{x}{\atom{one}} \}$
    rx = &mut x; // $\{ \absmap{x}{\loanm{l}}, \absmap{rx}{\borrowm{l}{\atom{one}}} \}$
    *rx = 'two; // $\{ \absmap{x}{\loanm{l}}, \absmap{rx}{\borrowm{l}{'two}} \}$
    // rx dropped
    x; // $\{ \absmap{x}{'two}, \absmap{rx}{\bot} \}$
  \end{minted}
  \caption{A reference to a variable being constructed. When the reference is dropped, the updated value from the mutable reference is written back to the original variable.}
\end{listing}

Mutable references are similar, except the value is also stored on the $\kw{loan}$, reflecting the fact that while an immutable loan exists, the value is still available in its original location. Having $\kw{loan}$ in an environment like this is also used to prevent mutations to a borrowed value.

\begin{listing}[H]
  \begin{minted}[mathescape]{rust}
    x = 'one; // $\{ x \mapsto \,'one \}$
    rx = &x; // $\{ x \mapsto \loans{l}{'one}, \,rx \mapsto \borrows{l}{'one} \}$
  \end{minted}
\end{listing}

Loans can be nested, which is useful when you want to temporarily give a value you have borrowed to something else.

\begin{listing}[H]
  \begin{minted}[mathescape]{rust}
    x = 'one; // $\{ \absmap{x}{one} \}$
    rx1 = &mut x; // $\{ \mono{x} \mapsto \loanm{l}, \,\mono{rx} \mapsto \borrowm{l}{'one} \}$
    rx2 = &mut *rx1; // $\{ \mono{x} \mapsto \loanm{l}, \,\mono{rx} \mapsto \borrowm{l}{(\loanm{l'})}, \mono{rx2} \mapsto \borrowm{l}{'one} \}$
  \end{minted}
  \caption{A reborrow}
\end{listing}

When immutable references are re-borrowed, the syntactic representation of the environment grows exponentially.

\begin{listing}[H]
  \begin{minted}[mathescape]{rust}
    x = 'one; // $\{ \absmap{x}{one} \}$
    rx1 = &x; // $\{ \mono{x} \mapsto \loans{l}{'one}, \,\mono{rx} \mapsto \borrows{l}{'one} \}$
    rx2 = &*rx1; // $\{ \mono{x} \mapsto \loans{l}{(\loans{l'}{'one})}, \,\mono{rx} \mapsto \borrows{l}{(\loans{l'}{'one})}, rx2 \mapsto \borrows{l'}{'one} \}$
    rx3 = &*rx2; // $\{ \mono{x} \mapsto \loans{l}{(\loans{l'}{(\loans{l''}{'one})})}, \,\mono{rx} \mapsto \borrows{l}{(\loans{l'}{(\loans{l''}{'one})})},$
                 // $\mono{rx2} \mapsto \borrows{l'}{(\loans{l''}{'one})}, \mono{rx3} \mapsto \borrows{l''}{'one} \}$
  \end{minted}
  \caption{An immutable re-borrow}
\end{listing}

This is not a problem for the implementation because the value stored in the $\kw{loan}$ and the value stored in the $\kw{borrow}$ are two pointers to the same underlying memory, it can just make working examples out by hand longer.

\subsubsection{(Dependent) Pairs}
In the abstract environment pairs store the type of the left side, and how to turn the type of the left side into the right, like so: $(\{'true, \, 'false\}, \mono{L} \rightarrow \mono{L})$. This reads "The left of the pair is of type $\{'true, \, 'false\}$, and the right is whatever the left is". This means in the future if the left is narrowed down to be $'true$, the right will be read as $'true$.

Non-dependent pairs are a special case of dependent pairs where the right happens to evaluate to the same type for any given left. A non-dependent pair of booleans would be constructed with \mono{(Bool, Bool)}, which is syntactic sugar for \mono{(Bool, \_ -> Bool)}.

\begin{listing}[H]
  \begin{minted}[mathescape]{rust}
    Bool = 'true | 'false;              // $\{ \absmap{Bool}{\{\atom{true}, \atom{false}\}} \}$
    BoolPair = (Bool, Bool);            // $\{ ..., \absmap{BoolPair}{(\{\atom{true}, \atom{false}\}, \mono{\_} \rightarrow \mono{Bool})} \}$
    Same = (Bool, L -> L);              // $\{ ..., \absmap{Same}{(\{\atom{true}, \atom{false}\}, \mono{L} \rightarrow \mono{L}} \}$
    specificPair = ('true, 'true);      // $\absenv{ ..., \absmap{specificPair}{(\atom{true}, \mono{\_} \rightarrow \mono{'true})}}$
    widenedPair = ('true, 'true): Same; // $\absenv{ ..., \absmap{widenedPair}{(\{\atom{true}, \atom{false}\}, \mono{L} \rightarrow \mono{L}}}$
  \end{minted}
  \caption{Various pair constructions and their respective entries in the abstract environment}
\end{listing}

Mutation breaks type dependencies across pairs. Once the left of a pair is mutated, the right must be generalized because the data is lost, meaning the programmer will never be able to recover which specific type the right had in the future.

\begin{listing}[H]
  \begin{minted}[mathescape]{rust}
    Same = ('true,  'true )
         | ('false, 'false);  // $\absenv{ \absmap{Same}{(\{\atom{true}, \atom{false}\}, \mono{L} \rightarrow \mono{L} )} }$
    p = ('true, 'true): Same; // $\absenv{ \absmap{Same}{...}, \absmap{p}{(\{\atom{true}, \atom{false}\}, \mono{L} \rightarrow \mono{L} )} }$
    p.0 = 'false;             // $\absenv{ \absmap{Same}{...}, \absmap{p}{(\atom{false}, \mono{\_} \rightarrow \mono{('true | 'false)} )} }$
    p.1 = 'false;             // $\absenv{ \absmap{Same}{...}, \absmap{p}{(\atom{false}, \mono{\_} \rightarrow \mono{'false})} }$
    p: Same;                  // $\absenv{ \absmap{Same}{...}, \absmap{p}{(\{\atom{true}, \atom{false}\}, \mono{L} \rightarrow \mono{L} )} }$
  \end{minted}
  \caption{Demonstration of how mutation interacts with dependent pairs. On line 4 when the left of the pair is mutated, the dependence is broken. When the right is mutated to $\atom{false}$, the pair's type is narrowed down, but it doesn't regain the dependence until the programmer explicitly widens the type on line 6.}
  \label{lst:mutpairs}
\end{listing}

Listing \ref{lst:mutpairs} depicts \mono{('true, 'true) | ('false, 'false)} being evaluated to $(\{\atom{true}, \atom{false}\}, \mono{L} \rightarrow \mono{L})$, which isn't strictly true. Type union between pairs will make the right depend on the left by producing a case statement for each of the possible left atoms, so \mono{('true, 'true) | ('false, 'false)} would instead evaluate to $(\{\atom{true}, \atom{false}\}, \mono{L} \rightarrow \mono{case L \{ 'true => 'true, 'false => 'false \}})$. In code examples it often evaluates to the former, to aid readability.

\subsubsection{Type Annotations}
Sometimes you want to manually manipulate what type the abstract interpretation reads from a piece of syntax. You do this with type annotations like \mono{M:T}. Evaluating a piece of syntax like \mono{M: T} both asserts that type of M is a subtype of T and makes the expression be of type T instead of M.

\begin{listing}[H]
  \begin{minted}[mathescape]{rust}
    x = 'true; // $\absenv{ \absmap{x}{\atom{true}}}$
    y = 'true: 'true | 'false; // $\absenv{..., \absmap{y}{\{\atom{true}, \atom{false}\}}}$
  \end{minted}
  \caption{The type annotation has caused type information to be lost: both \mono{x} and \mono{y} are set to $\atom{true}$ in the above code, but the type annotation on \mono{y} has caused the abstract interpretation to only be able to assign the wider type of $\{ \atom{true}, \atom{false} \}$}
\end{listing}

\subsubsection{Comptime vs Runtime}
As you will see in Section \ref{section:abstractinterpretation}, there are large differences in how the abstract interpretation is performed on runtime and comptime terms; however, for the most part, they map very similarly to the abstract environment. Following from the syntax level distinction, an entry in the abstract environment is marked as runtime or comptime by the variable identifier being capitalized or not.

\begin{listing}[H]
  \begin{minted}[mathescape]{rust}
    x = 'one; // $\absenv{ \absmap{x}{\atom{one}} }$
    X = 'one; // $\absenv{ ..., \absmap{X}{\atom{one}} }$
  \end{minted}
\end{listing}

One place differences do show is that runtime variables can mutate and be moved, whereas comptime values are immutable and can be freely used like values in typical pure functional languages.

This is so the programmer doesn't have to deal with manual memory management of comptime values, which they wouldn't benefit from anyway because all comptime variables are erased by the time the code is executed.

\begin{listing}[H]
  \begin{minted}[mathescape]{rust}
    x = 'runtime; // $\absenv{ \absmap{x}{\atom{runtime}} }$
    y = x; // $\absenv{ \absmap{x}{\bot}, \absmap{y}{\atom{runtime}}}$
  
    X = 'comptime; // $\absenv{ ..., \absmap{X}{\atom{comptime}} }$
    Y = X; // $\absenv{ ..., \absmap{X}{\atom{comptime}}, \absmap{Y}{\atom{comptime}} }$
  \end{minted}
  \caption{Unlike the runtime variable \mono{x}, which becomes uninitialized after being moved to \mono{y}, the comptime variable \mono{X} remains accessible while the value is simultaneously used by \mono{Y}, as you would expect from languages move semantics like Haskell}
\end{listing}

\subsubsection{Functions}
When a function is called, two things need to be calculated at the call site: whether or not the argument the programmer supplied is a subtype of the required argument; and what the return type is given this argument type. To achieve this we store two pieces of syntax, the input syntax and the return type syntax. We store syntax instead of types so the return type can depend on the input type.

\begin{listing}[H]
  \begin{minted}[mathescape]{rust}
    Bool = 'true | 'false; // $\absenv{ \absmap{Bool}{\{\atom{true}, \atom{false}\}} }$
    id = (b: Bool) -> Bool {
      b // $\absenv{ \absmap{Bool}{...}, \absmap{id}{\top}, \absmap{b}{\{\atom{true}, \atom{false}\}} }$
    } // $\absenv{ \absmap{Bool}{\{\atom{true}, \atom{false}\}}, \absmap{id}{\mono{(b: Bool)} \rightarrow \mono{Bool}} }$
  \end{minted}
  \caption{While type checking the body, argument \mono{b} is in the abstract environment. Abstractly the function is two pieces of \textit{syntax}: \mono{(b: Bool)} and \mono{Bool} instead of their respective types which are both $\{\atom{true}, \atom{false}\}$}
\end{listing}



\subsubsection{Case Statements}
In each of the branches of a case statement, the type of the interrogant is narrowed down to a specific atom. This is useful when the case is branching over the left of a dependent pair, because when the left of the pair gets narrowed down, so does the right\footnote{The right only gets narrowed once it is accessed, not immediately when the left is narrowed}.

Each branch of the case statement will modify the environment in some possibly different way. These are combined into one environment via an environment-wide union operation, which is the output environment.

\begin{listing}[H]
  \begin{minted}[mathescape]{rust}
    f = (b: 'true | 'false) -> 'unit {
                        // $\absenv{ \absmap{b}{\{\atom{true}, \atom{false}\}} }$
      case b {
        'true => (      // $\absenv{ \absmap{b}{\atom{true}} }$
          x = 'hello;   // $\absenv{ \absmap{b}{\atom{true}}, \absmap{x}{\atom{hello}} }$
        ),
        'false => (     // $\absenv{ \absmap{b}{\atom{false}} }$
          x = 'world;   // $\absenv{ \absmap{b}{\atom{false}}, \absmap{x}{\atom{world}} }$
          b = 'true;    // $\absenv{ \absmap{b}{\atom{true}}, \absmap{x}{\atom{world}} }$ 
        )
      };                // $\absenv{ \absmap{b}{\atom{true}}, \absmap{x}{\{ \atom{hello}, \atom{world} \}} }$
    }
  \end{minted}
  \caption{Each branch of the case statement is abstractly interpreted with \mono{b} narrowed down to a single atom (lines 4 and 7). Both branches modify \mono{x}, but to different values which make their environments different (lines 5 and 8); these different values are unioned together in the final environment to $\{\atom{hello}, \atom{world}\}$. The false branch happens to mutate \mono{b} back to $\atom{true}$, which means by the end of \textit{both} branches, $b: \atom{true}$, which is reflected in the final environment which maps \mono{b} to $\atom{true}$ instead of $\{\atom{true}, \atom{false}\}$.}
\end{listing}

\subsubsection{Complex Example Programs}
And last but not least, here are a few example programs which use several of the previous features together:

\cleardoublepage
% \section{Concrete Interpretation}
% \label{section:concreteinterpretation}
% The primary contribution of this research is the abstract interpretation because that is what will be used in the compiler for the type checking, but the regular interpretation is still useful to better understand the language features and to better understand the abstract interpretation. If you're only interested in how Ochre is type-checked, not its exact behavior at runtime, skip to Section \ref{section:abstractinterpretation}.

% Ochre has an extremely permissive syntax and heavily relies on typing rules to reject ill-formed programs. This leads to some usual results like \mono{(x = y) = z} being syntactically well-formed. In practice, this doesn't cause issues, because the typing rules are strict enough. The grammar for Ochre is shown in Figure \ref{fig:syntax}.

% We define two judgments over this syntax: $\Delta \vdash M \evalarrow v \dashv \Delta'$ and $\Delta \vdash M \ewritearrow v \dashv \Delta'$ where $\Delta$ and $Delta'$ are the stack states before and after, $M$ is a piece of syntax, and $v$ is a runtime value. The syntax for $\Delta$ and $v$ are given in Figure \ref{fig:stack}


% $M \evalarrow v$ defines what it means to evaluate a piece of syntax, and $M \ewritearrow v$ defines what it means to \textit{write} to a piece of syntax. Not all syntax can be written to, but parts can. This roughly equates to the concept of an lvalue in other languages.

% As an example: writing the value $\atom{five}$, to the syntax \mono{x} will mutate the entry for \mono{x} on the stack and set it to $\atom{five}$. Formally, this is expressed via the following definition:

% \begin{figure}[H]
%   \begin{gather*}
%     \inferrule{
%       \Delta' = \Delta\left[\dfrac{\mono{x} \mapsto v}{\absmap{x}{\top}}\right]
%     }{
%       \Delta \vdash \mono{x} \ewritearrow v \dashv \Delta'
%     }
%   \end{gather*}
%   \caption{Definition of $\ewritearrow$ for variable identifiers}
% \end{figure}

% This reads "the value $v$ is written to the syntax \mono{x} when the stack is $\Delta$, it will modify $\Delta$ such that \mono{x} now maps to $v$ instead of the previous $\bot$".

% For this rule to be applicable, \mono{x} must be uninitialized in the stack, which requires both that \mono{x} is in the stack at all, and that \mono{x} hasn't been written to previously, which is a very specific state for the stack to be in. There is a set of operations that can be performed at any time which perform things like memory allocation/deallocation, which one can use during derivations to get the stack into the right state, which will be covered later.

% Figures \ref{fig:concretesemanticsbi} and \ref{fig:concretesemanticsread} contain the concrete semantics for every piece of syntax. Often the rule for reading from a piece of syntax and writing to it differs only in which arrow is used; in these cases, I quantify over arrows using $\diamond$ to mean "either arrow".

% \begin{figure}
%   \centering
%   \begin{tabular}{c|cc}
%     & $\evalarrow$ & $\ewritearrow$ \\
%     \hline
%     \\\mono{x} &
%     \inferrule{
%       \absmap{x}{v} \in \Delta
%     }{
%       \Delta \vdash \mono{x} \evalarrow v
%     } &
%     \inferrule{
%       \Delta' = \Delta\left[\dfrac{\absmap{x}{v'}}{\absmap{x}{\bot}}\right]
%     }{
%       \Delta \vdash \mono{x} \ewritearrow v' \dashv \Delta'
%     } \\
    
%     \\\mono{'a} &
%     \multicolumn{2}{c}{
%       $\forall \diamond \in \{ \evalarrow, \ewritearrow \}. \left[
%         \inferrule{
%         }{
%           \Delta \vdash \mono{\atom{a}} \diamond \atom{a}
%         }
%       \right]$
%     } \\
    
%     \\$M\mono{,}N$ &
%     \multicolumn{2}{c}{
%       $\forall \diamond \in \{ \evalarrow, \ewritearrow \}. \left[
%         \inferrule{
%           \Delta \vdash M \diamond v \dashv \Delta'\\\\
%           \Delta' \vdash N \diamond w \dashv \Delta''
%         }{
%           \Delta \vdash M\mono{,}N \diamond (v, w) \dashv \Delta''
%         }
%       \right]$
%     }\\
    
%     \\\mono{$M$.0} &
%     \inferrule{
%       \Delta \vdash M \evalarrow (v, \_) \dashv \Delta'
%     }{
%       \Delta \vdash \mono{$M$.0} \evalarrow v \dashv \Delta'
%     } &
%     \inferrule{
%       \Delta \vdash M \evalarrow (\bot, w) \dashv \Delta' \\\\
%       \Delta' \vdash M \ewritearrow (v', w) \dashv \Delta''
%     }{
%       \Delta \vdash \mono{$M$.0} \evalarrow v' \dashv \Delta''
%     } \\
    
%     \\\mono{$M$.1} &
%     \inferrule{
%       \Delta \vdash M \evalarrow (\_, w) \dashv \Delta'
%     }{
%       \Delta \vdash \mono{$M$.1} \evalarrow w \dashv \Delta'
%     } &
%     \inferrule{
%       \Delta \vdash M \evalarrow (v, \bot) \dashv \Delta' \\\\
%       \Delta' \vdash M \ewritearrow (v, w') \dashv \Delta''
%     }{
%       \Delta \vdash \mono{$M$.1} \evalarrow w' \dashv \Delta''
%     } \\
    
%     \\\mono{*$M$} &
%     \multicolumn{2}{c}{
%     $\forall \diamond \in \{ \evalarrow, \ewritearrow \}. \left[
%       \inferrule{
%         \Delta \vdash M \evalarrow p \dashv \Delta' \\\\
%         \Delta' \vdash p \diamond v \dashv \Delta''
%       }{
%         \Delta \vdash \mono{*$M$} \diamond v \dashv \Delta''
%       }
%     \right]$
%     } \\
    
%     \\\mono{M:T} &
%     \multicolumn{2}{c}{
%     $\forall \diamond \in \{ \evalarrow, \ewritearrow \}. \left[
%       \inferrule{
%         \Delta \vdash M \diamond v \dashv \Delta' 
%       }{
%         \Delta \vdash \mono{M:T} \diamond v \dashv \Delta'
%       }
%     \right]$
%     } \\
    
%     \\\mono{\_} &
%     \multicolumn{2}{c}{
%     $\forall \diamond \in \{ \evalarrow, \ewritearrow \}. \left[
%       \inferrule{
%       }{
%         \Delta \vdash \mono{\_} \diamond \bot
%       }
%     \right]$
%     } \\
%   \end{tabular}
%   \caption{Concrete interpretation for the pieces of syntax which can be both read and written to.}
%   \label{fig:concretesemanticsbi}
% \end{figure}

% \begin{figure}
%   \centering
%   \begin{tabular}{p{3cm}|c}
%     & $\evalarrow$ \\
%     \hline
%     \\\mono{\&M} &
%     \inferrule{
%     }{
%       \Delta \vdash \mono{\&x} \evalarrow \mono{x}
%     }\,\,\,
%     \inferrule{
%       \Delta \vdash M \evalarrow p
%     }{
%       \Delta \vdash \mono{\&M.0} \evalarrow p.0
%     }\,\,\,
%     \inferrule{
%       \Delta \vdash M \evalarrow p
%     }{
%       \Delta \vdash \mono{\&M.1} \evalarrow p.1
%     }\\

%     \\\mono{\&\kw{mut}\,M} &
%     \inferrule{
%       \Delta \vdash \mono{\&M} \evalarrow p
%     }{
%       \Delta \vdash \mono{\&\kw{mut} M} \evalarrow p
%     }\\

%     \\\mono{M N} &
%     $\dfrac{
%       \begin{array}{cl}
%         \Delta \vdash M \evalarrow (\mono{I} \rightarrow \mono{O}) \dashv \Delta' & \ocomment{evaluate function} \\
%         \Delta' \vdash N \evalarrow n \dashv \Delta'' & \ocomment{evaluate argument} \\
%         \Delta'' \vdash \mono{I} \ewritearrow n \dashv \Delta''' & \ocomment{push argument to stack} \\
%         \Delta''' \vdash \mono{O} \evalarrow v \dashv \Delta'''' & \ocomment{execute function body}
%       \end{array}
%     }{
%       \Delta \vdash \mono{M N} \evalarrow v \dashv \Delta''''
%     }$ \\

%     \\\mono{M -> T\,\{\,N\,\}} &
%     \inferrule{
%     }{
%       \Delta \vdash \mono{M -> T \{ N \}} \evalarrow (M \rightarrow N)
%     }\\

%     \\\mono{M = N} &
%     \inferrule{
%       \Delta \vdash N \evalarrow v \dashv \Delta'\\\\
%       \Delta' \vdash M \ewritearrow v \dashv \Delta''
%     }{
%       \Delta \vdash \mono{M = N} \evalarrow \atom{unit} \dashv \Delta''
%     }\\

%     \\\mono{M; N} &
%     \inferrule{
%       \Delta \vdash M \evalarrow \atom{unit} \dashv \Delta'\\\\
%       \Delta' \vdash N \evalarrow n \dashv \Delta''
%     }{
%       \Delta \vdash \mono{M = N} \evalarrow n \dashv \Delta''
%     }\\

%     \\\mono{case M \{}\newline
%       \mono{  'a$_0$ => N$_0$,}\newline
%       \mono{  ...}\newline
%       \mono{  'a$_k$ => N$_k$,}\newline
%     \mono{\}} &
%     \raisebox{-6ex}{$\dfrac{
%       \begin{array}{cl}
%         \Delta \vdash M \evalarrow \atom{a}_i \dashv \Delta' & \ocomment{interrogant matches i$^\text{th}$ branch} \\
%         \Delta' \vdash N_i \evalarrow n_i \dashv \Delta'' & \ocomment{execute i$^\text{th}$ branch}
%       \end{array}
%     }{
%       \Delta \vdash \mono{case M \{ 'a$_0$\ => N$_0$\, ... \}} \evalarrow n_i \dashv \Delta''
%     }$} \\

%   \end{tabular}
%   \caption{Concrete interpretation for the pieces of syntax which can be evaluated but not written to.}
%   \label{fig:concretesemanticsread}
% \end{figure}

\cleardoublepage
\section{Formalisms}
\label{section:abstractinterpretation}
Type-checking is done via an abstract interpretation which takes a piece of Ochre syntax and outputs a type for that syntax while modifying the abstract environment. All interpretation rules take the form $\Omega \vdash M \diamond v \dashv \Omega'$ where $\diamond$ is one of $\{ \allarrows \}$, $M$ is Ochre syntax, $\Omega$ and $\Omega'$ are the abstract environments before and after, and $v$ is the type of the value which has been read or written.

Section \ref{subsection:modalities} introduces key concepts that are required to understand the definitions; subsequent subsections define the abstract interpretation. The reader is encouraged to skip around definitions a lot, they are laid out in tables to make finding a particular definition easier, as one might in a repository of code.

\subsection{Modalities}
\label{subsection:modalities}
It is best to conceptualize the arrows as a single interpretation with many modalities. The following arrows are used to denote the different modalities:

\def\arraystretch{1.5}
\begin{tabular}{c|l|l}
  & Read & Write \\
  \hline
  Runtime destructive & $\movearrow$ \ocomment{move}  & $\writearrow$ \ocomment{write}  \\
  Runtime non-destructive & $\readarrow$ \ocomment{read}  & $\narrowarrow$ \ocomment{type narrow} \\
  Comptime & $\erasedreadarrow$ \ocomment{erased read} &  $\erasedwritearrow$ \ocomment{erased write} \\
\end{tabular}
\def\arraystretch{1}

To summarize the modalities: a dot above the arrow denotes \textit{abstract} interpretation which means it is not executing the code, only type-checking it; squiggly arrows denote the interpretation of terms that are erased at compile time; arrows which point rightwards (away from the term) denote reads, arrows which point leftwards (towards the term) denote writes. These modalities are elaborated on below.

\subsubsection{Comptime vs Runtime Modality}
There are two distinct types of terms in Ochre: runtime terms, and comptime terms.

\textbf{Runtime terms} are constrained such that they can be executed efficiently on hardware. This involves manual memory management with move semantics, so no garbage collector is required at runtime, and allows the in-place mutation of data structures.

\textbf{Comptime terms} are erased at compile time and are only used to compute types. Because comptime terms only exist during compilation, and not during runtime, inefficient but automatic memory management strategies can be used, such as reference counting. This removes the need for move semantics, which allows types to be used multiple times without explicit copying.

Because comptime terms do not have move semantics, we cannot have mutation\footnote{The method of combining mutability with dependent types this research uses relies on move semantics, therefore we cannot have mutability on non-move semantics code.}, and we do not need immutable references. Not supporting mutation within comptime terms has the added benefit of the programmer not having to reason about the side effects of evaluating types, which happens implicitly in situations including type checking a function call site (see \odef{\movearrow}{M N}).
 
There is no syntactic distinction between comptime and runtime terms because they are so similar, although there could have been because one can always determine whether a term is comptime or runtime given it's position.

\subsubsection{Abstract vs Concrete Modality}
Comptime interpretations ($\erasedreadarrow, \erasedwritearrow$) are only ever abstract, whereas runtime interpretations ($\movearrow, \writearrow, \narrowarrow, \readarrow$) can have an optional dot above them; this dot means "abstract interpret".

Execution and type checking are very closely related in Ochre because execution is just the totally precise version of type checking. They differ only in how they treat type annotations: abstract interpretation will evaluate $M \mono{:} T$ to the result of evaluating $T$, and concrete interpretation will evaluate the same syntax to the result of evaluating $M$.

This guarantees concrete interpretation will output a precise result (singleton type) because the only way to form non-singleton types is via type union, which can only occur on the right hand side of a type annotation, because it's only permitted in comptime terms.

\subsubsection{Read vs Write Modality}
In typical languages like the $\lambda$-calculus, terms are evaluated to a value, which is equivilant to the notion of the read modality in Ochre. The write modality allows you to write a value to a term, which is how variables are brought into scope. You might find it useful to compare reading a variable $x$ with writing to a variable $x$:

\begin{figure}[H]
  \begin{mathpar}
    \inferrule[\odef{\movearrow}{x}]{
      \Omega' = \Omega\left[\frac{\absmap{x}{\top}}{\absmap{x}{v}}\right]
    }{
      \Omega \vdash x \movearrow m \dashv \Omega'
    }

    \inferrule[\odef{\writearrow}{x}]{
      \Omega' = \Omega\left[\frac{\absmap{x}{v}}{\absmap{x}{\top}}\right]
    }{
      \Omega \vdash x \writearrow v \dashv \Omega'
    } 
  \end{mathpar}
  \caption{Reading removes a value from the environment, whereas writing adds a value.}
\end{figure}
\vspace{-5ex}
\noindent
Defining the write operation for more complex pieces of syntax is how several language features are defined, including but not limited to: pattern matching, destructuring, and specifying function arguments.

% Explain all 6 by looking at the interpretations of the variable $x$.

% \subsection{Moving From Syntax}
% Destructive operations assert that a piece of syntax has a runtime influence on the data in memory, and updates the environment accordingly. As Ochre has Rust-like ownership semantics, so do these assertions; this means that reading from a variable will \textit{move} the value out of wherever it was previously, and prevent it from being used again. For example $\{x \mapsto 5\} \vdash x\,\dot{\Rightarrow}\,5 \dashv \{x \mapsto \bot\}$ means when $x$ is $5$ in the environment, you can read a $5$ from it, and can't read it again in the future. Writes are destructive because writing a value into memory requires the old value to be deallocated first. The full definition of $\dot{\Rightarrow}$

% \subsection{Writing To Syntax}
% The $\writearrow$ defines what it means to \textit{write} to a given piece of syntax. This is how values get brought into the context, for instance, an assign (\mono{=}) works by evaluating the RHS, and then writing it to the LHS, which is usually just a variable $x$, but in the case of destructuring could also be a pair $(x, y)$.



% \subsection{Reading from syntax}
% The $\readarrow$ defines what it means to read from a piece of syntax. Judgments of this form turn into memory reads at runtime, they can follow references and look into pairs.

% While most of the time this is used for reading, it can also be used to do something called type \textit{widening}. $\mono{x} \readarrow 5$ means 5 can be read from $\mono{x}$, but if 5 can be read from $\mono{x}$, so can $\mathbb{N}$, because $5$ is a subtype of $\mathbb{N}$.

% It's called type widening because it widens the type in the environment, instead of just reading a wider type while leaving the original alone. So $\{\mono{x} \mapsto 5\} \vdash \mono{x} \readarrow \mathbb{N} \dashv \{\mono{x} \mapsto \mathbb{N}\}$.

% If you encounter a judgment like $\Omega \vdash \mono{x} \readarrow 5$, it means the environment hasn't changed (i.e. $\Omega \vdash \mono{x} \readarrow 5 \dashv \Omega$), so you know the value of $\mono{x}$ has just been read, instead of being widened in place.

% The definition of $\readarrow$ for all pieces of syntax is shown in Figure \ref{fig:read}.


% \subsection{Type Narrowing}
% The $\narrowarrow$ constrains the type of syntax down to a smaller type. This is primarily useful when type-checking $\kw{case}$ expressions. When type-checking a particular branch of a case expression, you can modify the environment to reflect the fact that you know which branch you're in and therefore what the value of the case expression is. This is particularly useful when the type of the right-hand side of a pair depends on the left-hand value, because you can $\kw{case}$ the left-hand value, and in each branch you will know the type of the right. This is how ADTs are implemented in Ochre. It's definition is given in Figure \ref{fig:narrow}.


% Non-destructive operations

% Compile-time only operations

\FloatBarrier
\subsection{Syntax}
\FloatBarrier
Ochre grammar:

\begin{figure}[H]
  \arraycolsep=1pt %
  \centering

  \vspace{-2ex} %
  \[
  \begin{array}[t]{llll}
    S & ::= & & \ocomment{statement} \\
    & & M & \ocomment{expression} \\
    & & M = N; S & \ocomment{assignment} \\
    & & \mono{match}\,M\,\{\,\overrightarrow{M'\mono{ => }S}\,\} & \ocomment{match statement} \\
    \\
    T, U, M, N & ::= & & \ocomment{expression} \\
    && x \mid{} y \mid{} z  & \ocomment{runtime variable identifier} \\
    && X \mid{} Y \mid{} Z  & \ocomment{comptime variable identifier} \\
    && \mono{'}a & \ocomment{atom construction} \\
    && M\mono{,} N & \ocomment{pair construction} \\
    && M\mono{.0} & \ocomment{pair left access} \\
    && M\mono{.1} & \ocomment{pair right access} \\
    && \mono{*}M & \ocomment{dereference} \\
    && \mono{\&}M \mid \mono{\&mut}\,M & \ocomment{borrow constructor} \\
    && M N & \ocomment{function application} \\
    && M \, \mono{->} \, T \,(\mono{\{}\,N\,\mono{\}}) & \ocomment{function definition (optional runtime body)} \\
    && \mono{\_} & \ocomment{uninitialised} \\
    && T\,\mono{|}\,U & \ocomment{type union} \\
    && M\mono{:}\,T & \ocomment{type constraint} \\
    && v & \ocomment{type/value} \\
  \end{array} %
  \]
\caption{Ochre syntax} %
\label{fig:syntax} %
\end{figure} %

\textbf{Assignments never occur in a terminal position}. This avoids the question of what is the return value of an assignment.

\textbf{Match statements always occur in a terminal position}. RustBelt's $\lambda_Rust$ \cite{jungRustBeltSecuringFoundations2018a} has the same restriction, but Aeneas' LLBC \cite{aeneas} does not. If it was permitted for operations to occur after a match statement, the environment \textit{after} the match statement would have to be calculated, which would be the type union of the environments produced by the branches. For this type union operation to be precise over environments, like it is on pairs, we would have to support dependencies between variables. We do not support such dependencies for the sake of simplicity, and thus cannot precisely define environment union.

This restriction does not limit what programs can be represented because a program with match statements in non-terminal positions can be re-written to one that only has matches in terminal positions. This could be done by moving everything after the match statement into each of the match statement branches:

\begin{figure}[H]
  \begin{mathpar}
    \forall \diamond \{ \movearrow, \erasedreadarrow \} . \left[
      \inferrule{
        \Omega \vdash \mono{match $M$ \{ $\overrightarrow{M' \mono{ => } S \mono{;} S'}$ \} } \diamond t
      }{
        \Omega \vdash \mono{match $M$ \{ $\overrightarrow{M' \mono{ => } S}$ \} }; S' \diamond t
      } 
    \right]
  \end{mathpar}
  \caption{A re-write rule which could enable matches in non-terminal positions.}
\end{figure}

This rewrite rule has not been included in Ochre because I intend to support dependence between variables in the future, and therefore precise environment union, which removes the need for the rewrite. This rewrite rule has the disadvantage of causing an exponential blowup in code size/interpretation derivation size.

Assignment and match statements are the only constructs that can narrow types in the environment, so by having them both in statement ($S$) instead of expression ($M$), we guarantee that expression evaluation only ever widens types. This simplifies type-checking expressions.

\textbf{Types/values can be treated as syntax}. This syntactic construction cannot be constructed by the programmer. It is constructed internally within the interpretations to make syntax that always returns the same type/value, such as in \odef{\movearrow}{M\mono{.0}} where it is used to break the dependence of the pair so the left value can be moved out.

\FloatBarrier
\subsection{Abstract Environment/Values}
\FloatBarrier
The abstract environment, and abstract values:

\begin{figure}[H]
  \arraycolsep=1pt %
  \centering

  \vspace{-2ex} %
  \[
  \begin{array}[t]{llll}
    \Omega & ::= & & \ocomment{abstract environment (stack)} \\
    && \emptyset & \ocomment{empty stack} \\
    && \Omega, x \mapsto v & \ocomment{runtime variable} \\
    && \Omega, X \mapsto v & \ocomment{comptime variable} \\
    && \Omega, l \mapsto v & \ocomment{loan restriction} \\
    \\
    m, n, v, w, t, u & ::= & & \ocomment{type/value} \\
    && \{\overrightarrow{\atom{a}}\} & \ocomment{atom} \\
    && (v, T \rightarrow U) & \ocomment{pair} \\
    && (T \rightarrow U) & \ocomment{function} \\
    && \borrows{l}{v} \mid \borrowm{l}{v} & \ocomment{reference} \\
    && \loans{l}{v} \mid \loanm{l} & \ocomment{referenced value} \\
    && \top & \ocomment{top} \\
    % \\
    % p & ::= & & \ocomment{pointer} \\
    % && x & \ocomment{variable} \\
    % && p.0 \mid p.1 & \ocomment{pair element access} \\
  \end{array} %
  \]
\caption{Abstract/Concrete Environment and Types} %
\label{fig:stack} %
\end{figure} %

\subsubsection{The Top Type}
The $\top$ type is used to denote a lack of typing information. Every type/value is of type $\top$. When you move a value, the previous location is set to $\top$, to denote uninitialized data. When a value has never been writen to before, it's value is $\top$, again, to denote uninitialised data. When a type $t$ depends on another type $u$, but $u$ has not been narrowed down enough to deduce the type of $t$, $t$ evaluates to $\top$ to denote the lack of typing information.

\subsubsection{Concrete Values}
If a type is a singleton type (a type with one inhabitant), it is referred to as a value. For example $\{ \atom{a} \}$ is a concrete value/type, but $\{ \atom{a}, \atom{b} \}$ is not. Figure \ref{fig:dropconcrete} shows the formal definition of concrete values. Concrete values and non-singleton types share a grammar because rules are typically generic over both and preserve a values concreteness, so combining them avoids the syntatic overhead of introducing an additional modality.

\subsubsection{Drop Operation}
When an operation is no longer used, it must be dropped. At runtime, dropping a value will free it's associated memory, allowing it to be used for other operations, which is why Ochre doesn't need a garbage collector. Dropping a reference to a value removes the restrictions created by that reference. Drop is defined in Figure \ref{fig:dropconcrete}.

\subsubsection{Environment Rearrangement}
At any point during a program interpretation, whether it be abstract or concrete interpretation, the environment can be \textit{rearranged}, a technique introduced by Aeneas \cite{aeneas}. Environment rearranges can be inserted before or after any of the interpretation judgements (as defined in Figure \ref{fig:environmentrearrangements}) and can perform one of the following operations:

\begin{enumerate}
  \item \textbf{Allocation} - Before a variable is used, including before it is first written to, it must be mapped to $\top$ in the environment. Allocation takes a variable previously not in the environment, and maps it to $\top$.
  \item \textbf{Deallocation} - Occasionally typing judgements will assert that a series of operations leave the environment back in its original state (see \odef{\movearrow}{\mono{$M$ -> $T$ \{ $N$ \}}}). In order to achieve this variables allocated in that series of operations must be deallocated.
  \item \textbf{Type Widening} - At any point during the interpretation it is valid to forget typing information. For example: if a value is known to be one of $\{ \atom{a}, \atom{b} \}$, it is valid to now consider it to be one of $\{ \atom{a}, \atom{b}, \atom{c} \}$.
  \item \textbf{Dropping} - Before deallocation, values must be dropped. This is achieved in derivations by inserting rearrangements which drop values.
\end{enumerate}

Environment rearrangement is defined in Figure \ref{fig:environmentrearrangements}.

\begin{figure}
  \centering
  \small
  \begin{mathpar}
    \inferrule[Allocation]{
      \Omega' = \Omega,\absmap{xX}{\top}
    }{
      \Omega \rearrangearrow \Omega'
    }

    \inferrule[Deallocation]{
      \Omega',\absmap{x}{\top} = \Omega
    }{
      \Omega \rearrangearrow \Omega'
    }

    \inferrule[Type-Widen]{
      \Omega' = \Omega \left[ \frac{\absmap{x}{v'}}{\absmap{x}{v}} \right]\\\\
      \Omega \vdash v \subtype v'
    }{
      \Omega \rearrangearrow \Omega'
    }

    \inferrule[Drop]{
      \Omega' = \Omega \left[ \frac{\absmap{x}{\top}}{\absmap{x}{v}} \right]\\\\
      \Omega' \vdash \drop{v} \dashv \Omega''
    }{
      \Omega \rearrangearrow \Omega''
    }
  \end{mathpar}
  \begin{mathpar}
    \forall \diamond \in \{ \allarrows \} \left[\vcenter{\inferrule[Rearrange-Before]{
      \Omega \rearrangearrow \Omega' \\\\
      \Omega' \vdash M \diamond v \dashv \Omega''
    }{
      \Omega \vdash M \diamond v \dashv \Omega''
    }}\right]

    \forall \diamond \in \{ \allarrows \} \left[\vcenter{\inferrule[Rearrange-After]{
      \Omega \vdash M \diamond v \dashv \Omega' \\\\
      \Omega' \rearrangearrow \Omega''
    }{
      \Omega \vdash M \diamond v \dashv \Omega''
    }}\right]
  \end{mathpar}
  \caption{Definition of environment rearrangement $\Omega \rearrangearrow \Omega'$. \mono{xX} denotes "either runtime or comptime variable".}
  \label{fig:environmentrearrangements}
\end{figure}

\begin{figure}
  \centering
  \small
  \begin{tabular}{c|cc}
  $v$ & $\Omega \vdash \drop{v} \dashv \Omega' $ & $\concrete{v}$ \\
  \hline

  \\$\{ \overrightarrow{\atom{a}} \}$ &
  \inferrule{
    \\
  }{
    \Omega \vdash \drop{\{\overrightarrow{\atom{a}}\}}
  } &
  \inferrule{
    \\
  }{
    \concrete{\{ \atom{a} \}}
  } \\
  \\$(v, T \rightarrow U)$ &
  \inferrule{
    \Omega \vdash T \erasedwritearrow v \dashv \Omega' \\\\
    \Omega' \vdash U \erasedreadarrow w \\\\
    \Omega \vdash \drop{v} \dashv \Omega'' \\\\
    \Omega'' \vdash \drop{w} \dashv \Omega'''
  }{
    \Omega \vdash \drop{(v, T \rightarrow U)} \dashv \Omega'
  } &
  \inferrule{
    \concrete{v} \\\\
    \concrete{(T \rightarrow U)}
  }{
    \concrete{(v, T  \rightarrow U)}
  } \\

  \\$(T \rightarrow U)$ &
  \inferrule{
    \\
  }{
    \Omega \vdash \drop{(T \rightarrow U)}
  } &
  \inferrule{
    \runtime{T} \\\\
    \runtime{U}
  }{
    \concrete{(T \rightarrow U)}
  } \\

  \\$\borrows{l}{v}$ &
  \inferrule{
    \Omega' = \Omega \left[ \frac{v}{\loans{l}{v}} \right ]
  }{
    \Omega \vdash \drop{(\borrows{l}{v})} \dashv \Omega'
  } &
  \inferrule{
    \concrete{v}
  }{
    \concrete{(\borrows{l}{v})}
  } \\

  \\$\borrowm{l}{v}$ &
  \inferrule{
    \Omega' = \Omega \left[ \frac{v}{\loanm{l}} \right ]
  }{
    \Omega \vdash \drop{(\borrowm{l}{v})} \dashv \Omega'
  } &
  \inferrule{
    \concrete{v}
  }{
    \concrete{(\borrowm{l}{v})}
  } \\

  \\$\loans{l}{v}$ &
  \inferrule{
    \Omega' = \Omega \left[ \frac{\top}{\borrows{l}{v}} \right ] \\\\
    \Omega' \vdash \drop{v} \dashv \Omega'' 
  }{
    \Omega \vdash \drop{(\borrows{l}{v})} \dashv \Omega''
  } &
  \inferrule{
    \concrete{v}
  }{
    \concrete{(\loans{l}{v})}
  } \\

  \\$\loanm{l}$ &
  \inferrule{
    \Omega' = \Omega \left[ \frac{\top}{\borrowm{l}{v}} \right ] \\\\
    \Omega' \vdash \drop{v} \dashv \Omega'' 
  }{
    \Omega \vdash \drop{(\borrowm{l}{v})} \dashv \Omega''
  } &
  \inferrule{
    \\
  }{
    \concrete{(\loanm{l})}
  } \\

\end{tabular}
\caption{Definition of \drop{} and \concrete{}}
\label{fig:dropconcrete}
\end{figure}

\FloatBarrier
\subsection{Typing Judgements}
\FloatBarrier
The tables are to aid the reader to locate rules, they do not encode any information themselves.

These definitions are referenced with \odef{M}{\rightarrow}, where $\rightarrow$ is the arrow being defined, and $M$, is the piece of syntax it is being defined for. For example \odef{M N}{\movearrow} refers to the rule for destructively reading a function application.

\begin{figure}
  \begin{adjustwidth}{-1cm}{-1cm}
  \small
  \begin{tabular}{c|cccccc}
    & \erasedreadarrow & \readarrow & \movearrow & \writearrow & \narrowarrow & \erasedwritearrow \\
    \hline

    \\\mono{'a} &
    \multicolumn{6}{c}{
      $\forall \diamond \in \{ \erasedreadarrow, \readarrow, \movearrow, \writearrow, \narrowarrow, \erasedwritearrow \}. \left[
        \inferrule[]{
        }{
          \Omega \vdash \mono{'a} \diamond\,'a
        }
      \right]$
    }
    \\

    \\\mono{\_} &
    \multicolumn{6}{c}{
      $\forall \diamond \in \{ \allarrows \}. \left[
        \inferrule[]{
        }{
          \Omega \vdash \mono{\_} \diamond \top
        }
      \right]$
    }
    \\

    % \\\mono{*} &
    % \inferrule[]{
    % }{
    %   \Omega \vdash \mono{*} \erasedreadarrow \top
    % }&
    % &
    % &
    % &
    % &
    % \inferrule[]{
    % }{
    %   \Omega \vdash \mono{*} \erasedwritearrow \top
    % }\\

    \\\mono{M:T} &
    \multicolumn{3}{c}{
      $\forall \diamond \in \{ \erasedreadarrow, \readarrowabs, \movearrowabs \}. \left[
        \raisebox{-2ex}{$\dfrac{
          \begin{array}{cl}
            \Omega \vdash M \diamond m \dashv \Omega' \\
            \Omega' \vdash T \erasedreadarrow t \\
            \Omega' \vdash m \subtype t
          \end{array}
        }{
          \Omega \vdash M\mono{:}T \diamond t \dashv \Omega'
        }$}
      \right]$
    } &
    \multicolumn{3}{c}{
      $\forall \diamond \in \{ \erasedwritearrow, \narrowarrowabs, \writearrowabs \}. \left[
        \raisebox{-2ex}{$\dfrac{
          \begin{array}{cl}
            \Omega \vdash M \diamond m \dashv \Omega' \\
            \Omega' \vdash T \erasedwritearrow t \\
            \Omega' \vdash m \subtype t
          \end{array}
        }{
          \Omega \vdash M\mono{:}T \diamond t \dashv \Omega'
        }$}
      \right]$
    } \\

    \\ &
    &
    \multicolumn{4}{c}{
      $\forall \diamond \in \{ \concarrows \}. \left[
        \raisebox{-0ex}{$\dfrac{
          \begin{array}{cl}
            \Omega \vdash M \diamond m \dashv \Omega' & \ocomment{ignores type annotations} \\
          \end{array}
        }{
          \Omega \vdash M\mono{:}T \diamond m \dashv \Omega'
        }$}
      \right]$
    } &
    \\



  \end{tabular}
\end{adjustwidth}
\caption{everythingtable}
\label{fig:everythingtable}
\end{figure}

\begin{figure}
  \begin{adjustwidth}{-1cm}{-1cm}
  \small
  \begin{tabular}{c|ccc}
    $M$ & $\Omega \vdash M \movearrow v \dashv \Omega'$ & $\Omega \vdash M \readarrow v$ & $\Omega \vdash M \erasedreadarrow v$ \\
    \hline

    \\\mono{x} or \mono{X} &
    \inferrule[]{
      \Omega' = \Omega\left[\frac{\absmap{x}{\top}}{\absmap{x}{v}}\right]
    }{
      \Omega \vdash x \movearrow m \dashv \Omega'
    } &
    \inferrule[]{
      \absmap{x}{v} \in \Omega
    }{
      \Omega \vdash \mono{x} \readarrow v
    } &
    \inferrule[]{
      \absmap{X}{t} \in \Omega
    }{
      \Omega \vdash \mono{X} \erasedreadarrow t
    }
    \\

    \\$M\mono{,}N$ &
    \multicolumn{2}{c}{
      $\forall \diamond \in \{ \movearrow, \readarrow \}. \left[
        \inferrule[]{
          \Omega \vdash M \diamond m \dashv \Omega'\\\\
          \Omega' \vdash N \diamond n \dashv \Omega''
        }{
          \Omega \vdash \mono{($M$, $N$)} \diamond (m, \mono{\_} \rightarrow n) \dashv \Omega''
        }
      \right]$
    } &
    \inferrule{
      \Omega \vdash \mono{($T$, \_ -> $U$)} \erasedreadarrow v
    }{
      \Omega \vdash \mono{($T$, $U$)} \erasedreadarrow v
    } \\

    &
    &
    &
    \inferrule{
      \Omega \vdash T \erasedreadarrow t \\\\
      \Omega \vdash T' \erasedwritearrow t \dashv \Omega' \\\\
      \Omega' \vdash U \erasedreadarrow u
    }{
      \Omega \vdash \mono{($T$, $T'$ -> $U$)} \erasedreadarrow (t, T' \rightarrow U)
    } \\


    \\\mono{$M$.0} &
    \inferrule[]{
      \Omega \vdash M \movearrow (v, T \rightarrow S_t) \dashv \Omega'\\\\
      \Omega' \vdash T \erasedwritearrow v \dashv \Omega''\\\\
      \Omega'' \vdash S_t \erasedreadarrow w \\\\
      \Omega' \vdash M \writearrow (\top, \mono{\_} \rightarrow w) \dashv \Omega''
    }{
      \Omega \vdash \mono{$M$.0} \movearrow m \dashv \Omega''
    } &
    \multicolumn{2}{c}{
      $\forall \diamond \in \{ \readarrow, \erasedreadarrow \}. \left[
        \inferrule[]{
          \Omega \vdash M \diamond (m, \_ \rightarrow \_) \dashv \Omega' 
        }{
          \Omega \vdash \mono{$M$.0} \diamond m \dashv \Omega'
        }
      \right]$
    }
    \\
    
    \\\mono{$M$.1} &
    \multicolumn{3}{c}{
      $\forall \diamond \in \{ \movearrow, \readarrow, \erasedreadarrow \}. \left[
        \raisebox{-5.5ex}{$\dfrac{
          \begin{array}{lll}
            \Omega \vdash M \diamond (v, T \rightarrow U) \dashv \Omega' & \ocomment{get pair} & \\
            \Omega' \vdash T \erasedwritearrow v \dashv \Omega'' & \ocomment{calculate right restriction} & \\
            \Omega'' \vdash U \erasedreadarrow w \dashv \Omega' & \ocomment{calculate right restriction} & \\
            \Omega' \vdash M \writearrow (m, \mono{\_} \rightarrow \mono{\_}) \dashv \Omega''' & \ocomment{replace pair} & \textit{if $\diamond = \movearrow$} \\
            \Omega''' = \Omega' & & \textit{if $\diamond \ne \movearrow$}
          \end{array}
        }{
          \Omega \vdash \mono{$M$.1} \diamond w \dashv \Omega'''
        }$}
      \right]$
    }
    \\
    
    % \\\mono{$M$.1} &
    % \multicolumn{3}{c}{
    %   $\forall \diamond \in \{ \movearrow, \readarrow, \erasedreadarrow \}. \left[
    %     \vcenter{\inferrule{
    %       {\begin{aligned}
    %         & \Omega \vdash M \diamond (v, T \rightarrow U) \dashv \Omega' & & \ocomment{get pair} & \\
    %         & \Omega' \vdash T \erasedwritearrow v \dashv \Omega'' & & \ocomment{calculate right restriction} & \\
    %         & \Omega'' \vdash U \erasedreadarrow w \dashv \Omega' & & \ocomment{calculate right restriction} & \\
    %         & \Omega' \vdash M \writearrow (m, \mono{\_} \rightarrow \mono{\_}) \dashv \Omega''' & & \ocomment{replace pair} & \textit{if $\diamond = \movearrow$} \\
    %         & \Omega''' = \Omega' & & & \textit{if $\diamond \ne \movearrow$}
    %       \end{aligned}}
    %     }{
    %       \Omega \vdash \mono{$M$.1} \diamond w \dashv \Omega'''
    %     }}
    %   \right]$
    % }
    % \\
    
    %% MAKES LATEX HANG
    % \\\mono{$M$.1} &
    % \multicolumn{3}{c}{
    %   $\forall \diamond \in \{ \movearrow, \readarrow, \erasedreadarrow \}. \left[
    %     \vcenter{{\begin{array}{ccccc}
    %       & \Omega \vdash M \diamond (v, T \rightarrow U) \dashv \Omega' & & \ocomment{get pair} & \\
    %       & \Omega' \vdash T \erasedwritearrow v \dashv \Omega'' & & \ocomment{calculate right restriction} & \\
    %       & \Omega'' \vdash U \erasedreadarrow w \dashv \Omega' & & \ocomment{calculate right restriction} & \\
    %       & \Omega' \vdash M \writearrow (m, \mono{\_} \rightarrow \mono{\_}) \dashv \Omega''' & & \ocomment{replace pair} & \textit{if $\diamond = \movearrow$} \\
    %       & \Omega''' = \Omega' & & & \textit{if $\diamond \ne \movearrow$}
    %       \end{aligned}}
    %       \hline
    %       \Omega \vdash \mono{$M$.1} \diamond w \dashv \Omega'''
    %     \end{array}}
    %   \right]$
    % }
    % \\

    \\\mono{*$M$} &
    &
    \inferrule[]{
      \Omega \vdash M \readarrow \borrows{l}{v}
    }{
      \Omega \vdash \mono{*$M$} \readarrow v
    }
    \\

    \\ &
    \inferrule[]{
      \Omega \vdash M \movearrow \borrowm{l}{v} \dashv \Omega'\\\\
      \Omega' \vdash M \writearrow \borrowm{l}{\bot} \dashv \Omega''
    }{
      \Omega \vdash \mono{*$M$} \movearrow v \dashv \Omega''
    } &
    \inferrule[]{
      \Omega \vdash M \readarrow \borrowm{l}{v}
    }{
      \Omega \vdash \mono{*$M$} \readarrow v
    }
    \\

  \end{tabular}
\end{adjustwidth}
\caption{readtable}
\label{fig:readtable}
\end{figure}

\begin{figure}
  \begin{adjustwidth}{-1cm}{-1cm}
  \small
  \centering
  \begin{tabular}{c|ccc}
    $M$ & $\Omega \vdash M \writearrow v \dashv \Omega' $ & $\Omega \vdash M \narrowarrow v \dashv \Omega' $ & $\Omega \vdash T \erasedwritearrow t \dashv \Omega' $ \\
    \hline

    \\\mono{x} or \mono{X} &
    \inferrule[]{
      \Omega' = \Omega\left[\frac{\absmap{x}{v}}{\absmap{x}{\top}}\right]
    }{
      \Omega \vdash x \writearrow v \dashv \Omega'
    } &
    \inferrule[]{
      \Omega' = \Omega\left[\frac{\absmap{x}{v'}}{\absmap{x}{v}}\right]\\\\
      m':m
    }{
      \Omega \vdash x \narrowarrow m' \dashv \Omega'
    } &
    \inferrule[]{
      \Omega' = \Omega\left[\frac{\absmap{X}{v'}}{\absmap{X}{v}}\right]\\\\
      m':m
    }{
      \Omega \vdash X \erasedwritearrow m' \dashv \Omega'
    }
    \\

    \\$M\mono{,}N$ &
    \multicolumn{3}{c}{
      $\forall \diamond \in \{ \writearrow, \narrowarrow, \erasedwritearrow \}. \left[
        \vcenter{\inferrule[]{
          \Omega \vdash T \erasedwritearrow m \dashv \Omega'\\\\
          \Omega' \vdash U \erasedreadarrow n\\\\
          \Omega \vdash M \diamond m \dashv \Omega''\\\\
          \Omega'' \vdash N \diamond n \dashv \Omega'''
        }{
          \Omega \vdash M\mono{,}N \,\diamond\, (m, T \rightarrow U) \dashv \Omega'''
        }}
      \right]$
    }
    \\

    \\\mono{$M$.0} &
    \inferrule[]{
      \Omega \vdash M \writearrow (\bot, n, \mono{\_} \rightarrow U) \dashv \Omega'\\\\
      \Omega' \vdash M \writearrow (m, T \rightarrow U) \dashv \Omega''
    }{
      \Omega \vdash \mono{$M$.0} \writearrow m \dashv \Omega''
    } &
    \multicolumn{2}{c}{
      $\forall \diamond \in \{ \narrowarrow, \erasedwritearrow \}. \left[
        \vcenter{\inferrule[]{
          \Omega \vdash M \diamond (\bot, n, \mono{\_} \rightarrow U) \dashv \Omega'
        }{
          \Omega \vdash \mono{$M$.0} \diamond m \dashv \Omega''
        }}
      \right]$
    }
    \\

    \\\mono{$M$.1} &
    \inferrule[]{
      \Omega \vdash M \writearrow (m, \bot, T \rightarrow \mono{\_}) \dashv \Omega'\\\\
      U = \kw{translate} \, n \\\\
      \Omega' \vdash M \writearrow (m, T \rightarrow U) \dashv \Omega''
    }{
      \Omega \vdash \mono{$M$.1} \writearrow n \dashv \Omega''
    }
    \\

    \\\mono{\_} &
    \inferrule[]{
      \Omega \vdash \drop{v} \dashv \Omega'
    }{
      \Omega \vdash \mono{\_} \writearrow v \dashv \Omega'
    } &
    \multicolumn{2}{c}{
      $\forall \diamond \in \{ \narrowarrow, \erasedwritearrow \}. \left[
        \inferrule[]{
        }{
          \Omega \vdash \mono{\_} \readarrow \bot  
        }
      \right]$
    }
    \\

    \\\mono{*}
    \\

    \\\mono{*$M$} &
    \inferrule[]{
      \Omega \vdash M \movearrow \borrowm{l}{\bot} \dashv \Omega'\\\\
      \Omega' \vdash M \writearrow \borrowm{l}{v} \dashv \Omega''
    }{
      \Omega \vdash \mono{*$M$} \writearrow v \dashv \Omega''
    } &
    \inferrule[]{
      \Omega \vdash M \readarrow \borrows{l}{v'} \dashv \Omega'\\\\
      \Omega'' = \Omega'[\loans{l}{v'}/\loans{l}{v}]\\\\
      v':v
    }{
      \Omega \vdash \mono{*$M$} \narrowarrow v' \dashv \Omega'
    }
    \\

  \end{tabular}
\end{adjustwidth}
\caption{writetable}
\label{fig:writetable}
\end{figure}

\begin{figure}
  \begin{adjustwidth}{-1cm}{-1cm}
  \small
  \begin{tabular}{c|ccc}
    $M$ & $\Omega \vdash M \movearrow v \dashv \Omega'$ & $\Omega \vdash T \erasedreadarrow t$ \\
    \hline

    \\\mono{\&}M &
    \inferrule[]{
      \Omega \vdash M \readarrow m\\\\
      \Omega \vdash M \narrowarrow \loans{l}{m} \dashv \Omega'
    }{
      \Omega \vdash \mono{\&}M \movearrow \borrows{l}{m} \dashv \Omega'
    }
    \\

    \\\mono{\&mut} M &
    \inferrule[]{
      \Omega \vdash M \movearrow m \dashv \Omega'\\\\
      \Omega' \vdash M \writearrow \kw{loan}^m\,l \dashv \Omega''
    }{
      \Omega \vdash \mono{\&mut} M \movearrow \kw{borrow}^m\,l\,m \dashv \Omega''
    }
    \\

    \\M N &
    $\dfrac{
      \begin{array}{ll}
        \Omega \vdash F \readarrowabs (M \rightarrow S_t) & \ocomment{eval function} \\
        \Omega \vdash X \movearrowabs v \dashv \Omega' & \ocomment{eval argument} \\
        \Omega' \vdash T \writearrowabs v \dashv \Omega'' & \ocomment{calculate return type} \\
        \Omega'' \vdash S_t \erasedreadarrow w & \ocomment{calculate return type} \\
        \multicolumn{2}{c}{\text{maximally widen $v$ to $v'$ such that $T \erasedwritearrow v'$}} \\
        \Omega' \vdash \text{drop}\,v' \dashv \Omega''' & \ocomment{propogate side effects} \\
      \end{array}
    }{
      \Omega \vdash F X \movearrowabs w \dashv \Omega'''
    }$ &
    $\dfrac{
      \begin{array}{ll}
        \Omega \vdash F \erasedreadarrow (T \rightarrow U) & \ocomment{eval function} \\
        \Omega \vdash X \erasedreadarrow v & \ocomment{eval argument} \\
        \Omega \vdash T \erasedwritearrow v \dashv \Omega' & \ocomment{calculate return} \\
        \Omega' \vdash U \erasedreadarrow w' & \ocomment{calculate return}
      \end{array}
    }{
      \Omega \vdash F X \erasedreadarrow w
    }$ \\

    \\ &
    \inferrule{
      \Omega \vdash F \readarrowconc (M  \rightarrow S) \\\\
      \Omega \vdash X \movearrowconc v \dashv \Omega' \\\\
      \Omega' \vdash M \writearrowconc v \dashv \Omega'' \\\\
      \Omega'' \vdash S \movearrowconc w \dashv \Omega'''
    }{
      \Omega \vdash F X \movearrowconc w \dashv \Omega'''
    } \\

    \\\mono{$M$ -> $S_t$ $\left(\mono{\{}S\mono{\}}\right)$} &
    % \\M \mono{->} S_t (\mono{\{}S\mono{\}}) &
    $\dfrac{
      \begin{array}{ll}
        \Omega \vdash M \maxarrow{\erasedwritearrow} m \dashv \Omega' & \ocomment{calculate widest input} \\
        \Omega' \vdash S \subtype S_t \movearrowabs u & \ocomment{check body}
      \end{array}
    }{
      \Omega \vdash M \mono{->} S_t \mono{\{} S \mono{\}} \movearrowabs M \rightarrow S_t
    }$ &
    $\dfrac{
      \begin{array}{cl}
        \Omega \vdash T \maxarrow{\erasedwritearrow} t \dashv \Omega' & \ocomment{calculate widest input} \\
        \Omega' \vdash S_t \erasedreadarrow u & \ocomment{check body}
      \end{array}
    }{
      \Omega \vdash T \mono{->} S_t \erasedreadarrow T \rightarrow S_t
    }$ &
    \\

    \\ &
    \inferrule{
      \\
    }{
      \Omega \vdash M \mono{->} S_t \mono{\{} S \mono{\}} \movearrowconc M \rightarrow S
    } \\

    \\T \mono{|} U &
    &
    \inferrule[]{
      \Omega \vdash T \erasedreadarrow t \dashv \Omega'\\\\
      \Omega' \vdash U \erasedreadarrow u \dashv \Omega''
    }{
      \Omega \vdash T \mono{|} U \erasedreadarrow t \cup u \dashv \Omega''
    }
    \\

\end{tabular}
\end{adjustwidth}
\caption{readonlytable}
\label{fig:readonlytable}
\end{figure}

\begin{figure}
  \begin{adjustwidth}{-1cm}{-1cm}
  \small
  \centering
  \begin{tabular}{p{2.5cm}|cc}
    $S$ & $\Omega \vdash S \subtype S_t \movearrow v$ & $\Omega \vdash S \erasedreadarrow t$ \\
    \hline

    \\$M$ &
    $\raisebox{-5ex}{$\dfrac{
      \begin{array}{cl}
        \Omega \vdash S_t \erasedreadarrow t \\
        \Omega \vdash M \movearrow v \dashv \Omega' \\
        \Omega' \vdash \kw{drop} \\
        v \subtype t
      \end{array}
    }{
      \Omega \vdash M \subtype S_t \movearrow w
    }$}$ &
    \raisebox{-5ex}{$\dfrac{
      \begin{array}{cl}
        \Omega \vdash T \erasedreadarrow t \dashv \Omega' \\
        \Omega' \vdash \kw{drop}
      \end{array}
    }{
      \Omega \vdash T \erasedreadarrow t
    }$}  \\

    \\$M \mono{=} N \mono{;} S$ &
    \raisebox{-6ex}{$\dfrac{
      \begin{array}{cl}
        \Omega \vdash N \erasedreadarrow v \dashv \Omega' \\
        \Omega' \vdash M \erasedwritearrow v \dashv \Omega'' \\
        \Omega \vdash S_t \erasedreadarrow t \\
        \Omega'' \vdash S_t \erasedreadarrow t' \\
        t' \subtype t \\
        \Omega'' \vdash S \subtype S_t \movearrow w
      \end{array}
    }{
      \Omega \vdash M \mono{=} N \mono{;} S \subtype S_t \movearrow w
    }$} &
    \raisebox{-6ex}{$\dfrac{
      \begin{array}{cl}
        \Omega \vdash N \erasedreadarrow v \dashv \Omega' \\
        \Omega' \vdash M \erasedwritearrow v \dashv \Omega'' \\
        \Omega'' \vdash S_t \erasedreadarrow w
      \end{array}
    }{
      \Omega \vdash M \mono{=} N \mono{;} S_t \erasedreadarrow w
    }$} \\
    % \multicolumn{2}{c}{
    %   $\forall \diamond \in \{ \movearrow, \erasedreadarrow \} \left[
    %     \raisebox{-6ex}{$\dfrac{
    %       \begin{array}{cl}
    %         \Omega \vdash N \erasedreadarrow v \dashv \Omega' \\
    %         \Omega' \vdash M \erasedwritearrow v \dashv \Omega'' \\
    %         \Omega \vdash S_t \erasedreadarrow t \\
    %         \Omega'' \vdash S_t \erasedreadarrow t' \\
    %         t' \subtype t \\
    %         \Omega'' \vdash S \subtype S_t \diamond w
    %       \end{array}
    %     }{
    %       \Omega \vdash M \mono{=} N \mono{;} S \subtype S_t \diamond w
    %     }$}
    %   \right]$
    % } \\

    \\$M \mono{:=} N \mono{;} S$ &
    $\vcenter{\inferrule[]{
      \Omega \vdash N \movearrow v \dashv \Omega' \\\\
      \Omega' \vdash M \writearrow v \dashv \Omega'' \\\\
      \Omega \vdash S_t \erasedreadarrow t \\\\
      \Omega'' \vdash S_t \erasedreadarrow t' \\\\
      t' \subtype t \\\\
      \Omega'' \vdash S \subtype S_t \movearrow w
    }{
      \Omega \vdash M \mono{=} N \mono{;} S \subtype S_t \movearrow w
    }}$ \\

    \\\mono{match $M$ \{}\newline
      \mono{  }$M'_0$ \mono{=>} $S_0$ \mono{,} \newline
      \mono{    }\vdots\newline
      \mono{  }$M'_k$ \mono{=>} $S_k$ \mono{,} \newline
    \mono{\}}&
    \multicolumn{2}{c}{
      $\forall \diamond \in \{ \movearrow, \erasedreadarrow \}. \left[
        \raisebox{-5ex}{$\dfrac{
          \begin{array}{cclc}
            & \Omega \vdash M \diamond m \dashv \Omega' & \ocomment{eval interogant} & \\
            \forall i.[ & \Omega' \vdash M \writearrow m_i \dashv \Omega'_i & \ocomment{branch input} & ] \\
            \forall i.[ & \Omega'_i \vdash S_i : S_t \diamond n_i & \ocomment{branch output} & ] \\
            & n = \bigsqcup_i [ n_i ]\\
          \end{array}
        }{
          \Omega \vdash \mono{case} \, M \mono{\{} 'a_0 \mono{=>} S_0, ... \mono{\}} : S_t \diamond n \dashv \Omega''
        }$}
      \right]$
    }
    \\

  \end{tabular}
\end{adjustwidth}
\caption{statementtable}
\label{fig:statementtable}
\end{figure}

\FloatBarrier
\subsection{Type Operations}

\begin{figure}[H]
  \begin{adjustwidth}{-1cm}{-1cm}
  \small
  \centering
  \begin{tabular}{c|ccc}
    $v$ & $\Omega \vdash t \subtype u $ & $\Omega \vdash v = t \typeunion u$ & $\Omega \vdash v = t \typeinter u$ \\
    \hline

    \\$\{ \overrightarrow{\atom{a}} \}$ &
    \inferrule{
      \{ \overrightarrow{\atom{a}} \} \subseteq  \{ \overrightarrow{\atom{b}} \}
    }{
      \Omega \vdash \{ \overrightarrow{\atom{a}} \} \subtype \{ \overrightarrow{\atom{b}} \}
    } &
    \inferrule{
      v = \{ \overrightarrow{\atom{a}} \} \uplus  \{ \overrightarrow{\atom{b}} \}
    }{
      \Omega \vdash v = \{ \overrightarrow{\atom{a}} \} \typeunion \{ \overrightarrow{\atom{b}} \}
    } &
    \inferrule{
      v = \{ \overrightarrow{\atom{a}} \} \cap  \{ \overrightarrow{\atom{b}} \}
    }{
      \Omega \vdash v = \{ \overrightarrow{\atom{a}} \} \typeinter \{ \overrightarrow{\atom{b}} \}
    } \\

    \\$(v, T \rightarrow U)$ &
    \inferrule{
      \Omega \vdash t \subtype t' \\\\
      \Omega \vdash T' \erasedwritearrow t \dashv \Omega' \\\\
      \Omega' \vdash T \erasedwritearrow t \dashv \Omega'' \\\\
      \Omega'' \vdash S : S' \erasedreadarrow v
    }{
      \Omega \vdash (t, T \rightarrow S) \subtype (t', T' \rightarrow S')
    }&
    \inferrule{
      \Omega \vdash v'' = v \typeunion v' \\\\
      \Omega \vdash (T'' \rightarrow U'') = (T \rightarrow U) \typeunion (T' \rightarrow U') \\\\
      w = (v'', T'' \rightarrow U'')
    }{
      \Omega \vdash w = (v, T \rightarrow U) \typeunion (v', T' \rightarrow U')
    }
    \\

    \\$(T \rightarrow S)$ &
    \inferrule{
      \ocomment{note: function domains must be equal} \\\\
      \Omega \vdash T \maxarrow{\erasedwritearrow} t \dashv \Omega' \\\\
      % \Omega' \vdash T \erasedwritearrow t \dashv \Omega'' \\\\
      \Omega' \vdash S \subtype S' \erasedreadarrow v
    }{
      \Omega \vdash T \rightarrow S \subtype T \rightarrow S'
    } &
    \inferrule{
      v = (\mono{(L: $T$ | $T'$)} \rightarrow \mono{match L \{} \\\\
      \mono{    $T$ => $S$, $T'$ => $S'$ \}})
    }{
      \Omega \vdash v = (T \rightarrow S) \typeunion (T' \rightarrow S')
    }\\

    \\$\borrows{l}{v}$ \\

    \\$\borrowm{l}{v}$ \\

    \\$\loans{l}{v}$ \\

    \\$\loanm{l}$ \\


  \end{tabular}
\end{adjustwidth}
\caption{typeoperations}
\label{fig:typeoperations}
\end{figure}

\FloatBarrier
\subsection{Helper Definitions}
\FloatBarrier

\subsubsection{Arrow Designations}
\begin{figure}[H]
  \centering
  \small
  \begin{mathpar}
    \forall \diamond \in \{ \abstractarrows \} \left[
      \inferrule{
        \\
      }{
        \oabstract{\diamond}
      }\right]

      \forall \diamond \in \{ \concarrows \} \left[
        \inferrule{
          \\
        }{
          \concrete{\diamond}
        }
      \right]
  \end{mathpar}
  \caption{Helper designations for arrows}
  \label{fig:arrowhelper}
\end{figure}

\FloatBarrier

\section{Design Decisions}
\label{section:designdecisions}

\subsection{Justification of Feature Inclusion}
List of features, along with why their inclusion was important.

\subsubsection{Type Erasure}
A crucial goal of this project is to generate efficient machine code, so I don't want any aspect of the type system to influence runtime. It also ensures all reasoning about the program's correctness is done at compile time.

\subsubsection{Implementability}
The type system presented as a means to the end of making a production-ready language with sound foundations. If it relies too heavily on non-syntax-driven typing rules or extra information provided during the derivation, implementation could be rendered infeasible. An example of this is the $:$ operator, which asserts that the LHS has the type of the RHS; if this was just theory work I wouldn't need this because I could make these type assertions in the derivations.

\subsubsection{Manual memory management}
Manual memory management is important both toward the end of making efficient machine code, and dependent types. The real core of why dependent types are possible in this context is because safe Rust behaves very similarly to pure functional code behind the scenes, as demonstrated by the existence of multiple projects that can translate safe Rust into pure functional code \citep{aeneas}\citep{ullrichKhaElectrolysis2024}. The abstract interpretation introduced by Aeneas to track the state of ownership has proven crucial to detecting when typing judgments are invalidated by mutations.

\subsection{Justification of Feature Omission}
List of omitted features, along with why their omission is inconsequential for the conclusions of this research:

\subsubsection{Returning mutable references}
In Ochre you can put references in variables and pass them to functions, but you can never return them from a function. This doesn't restrict which programs you can express, because you can inline any function that would return a mutable reference and it will work, however, it does make using custom data structures like containers extremely cumbersome because you cannot define generic getters that return references to elements within the container. Supporting returning mutable references would involve introducing the concept of regions from Aeneas into Ochre, which I'm almost certain is possible, but would have complicated the already complicated type system.

\subsubsection{Reasoning about function side effects/strong updates}
In Ochre, if a function takes a mutable reference to a value of type $T$, the value is guaranteed to still be of type $T$ after the function return. You may want this not to be the case if the type encodes some property of your data structure, for instance, if you have a type for lists and another for sorted lists you may want an in-place sorting algorithm to change the type of the referenced list into a sorted list. I choose to not support this for a few reasons:
\begin{enumerate}
  \item I predict that it will be idiomatic in Ochre to separate data structures from proofs about their structure. If this is the case, you could return a proof about one of your inputs, which immutably borrows that input, causing it to be invalidated if the data structure is ever mutated. This would not involve strong updates.
  \item It would complicate the type system and syntax further.
  \item People can still do strong updates by moving the data structure in and out of a function instead of giving it a borrow. This is even possible if the caller only has a mutable reference to the data because strong updates are allowed locally.
\end{enumerate}

\subsubsection{Unboxed types}
All values in Ochre are one machine word long, which involves pairs being boxed. Unboxing data would require me to reason about the size of types at compile time, which would have complicated the type system further and detracted from the core contributions. Unboxing pairs should be very possible for Ochre in the future because it already has ownership and it will do generics via monomorphisation like Rust and C++. The complexity will arise because, unlike Rust, the type of data can change due to a mutation, and therefore its size. I will get around this via explicit boxing: a pointer to a heap allocation is always one machine word long, so you can change the size of the data behind it without changing the size of the data structure the pointer lies within. Unboxed types could be introduced in the future via the method laid out in Appendix \ref{appendix:unbox}.

\subsubsection{Primitive data types}
As presented, Ochre doesn't expose key data types such as machine integers which can be used to generate efficient arithmetic. This is a major problem for its short-term usefulness because all numeric arithmetic must be done with inefficient algorithms over heap-allocated Peano numbers. I think this is a reasonable omission because this work is mostly a proof of concept, and efficiently type-checking and compiling these primitives is well-explored and will be introduced into Ochre in the future.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Evaluation}
Ochre has two goals: to allow the programmer to encode strong properties in the type system, and to be efficiently executable on hardware. Section \ref{section:performance} evaluates Ochre against the former goal, and Section \ref{section:typesystem} evaluates against the latter.

A secondary goal of this research is to make a pleasant and powerful language with useful abstractions. This is discussed in Section \ref{section:ergonomics}.

The contribution of this research is the design and specification of a language, as opposed to an implementation, so the contributions will be evaluated against how well the design \& type checking algorithm lays the foundations for a future implementation.

\section{Type System}
\label{section:typesystem}
The type system presented must reject well-formed programs, and reject ill-formed ones. This section evaluates whether or not this is the case in two distinct ways: firstly Section \ref{section:examplechecks} shows the typing rules in action for a collection of programs with should or shouldn't compile. Then, Section \ref{section:properties} attempts to reason about what properties should and do hold.

\subsection{Example Type Checks}
\label{section:examplechecks}

\subsection{Properties \& Proofs}
\label{section:properties}
In theorems, unless otherwise specified, free variables are universally quantified.

This section aims to convince the reader that the following property holds:

\begin{theorem}{Statement interpretation is sound (when environment empty)} \[
  \oimplies{
    \inferrule{
      \emptyset \vdash S \subtype \_ \movearrowabs t \\\\
      \emptyset \vdash S \subtype \_ \movearrowconc v
    }{}
  }{
    \emptyset \vdash v: t
  }
\]\end{theorem}

Which reads ``If the abstract interpretation runs to $t$, and the concrete interpretation runs to $v$, then $v:t$''. It also reflects the fact that program execution always starts with a statement in an empty environment.

This soundness property assumes both \movearrowabs and \movearrowconc instead of only assuming the former and concluding the latter because that would equate to "if it type-checks, it executes" which is undecidable for turing complete languages \citep{turingComputableNumbersApplication1937}, which Ochre almost certainly is.

The proof is done by induction on the \movearrowabs derivation and the \movearrowconc derivation simultaneously. So that a stronger inductive hypothesis can be assumed, we prove this stronger property:

\begin{theorem}{Statement interpretation is sound} \[
  \oimplies{
    \inferrule{
    \Omega \vdash S \subtype \_ \movearrowabs t \\\\
    \Delta \vdash S \subtype \_ \movearrowconc v \\\\
    \Delta : \Omega
  }{}  
  }{
    \Omega \vdash v: t
  }
\]\end{theorem}

Which involves proving all of these properties:

\begin{theorem}{Expression interpretation is sound} \[
  \text{for all} \diamond \text{in} \{ \concarrows \} . \left[
    \oimplies{
      \inferrule{
      \Omega \vdash S \subtype \_ \,\dot{\diamond}\, t \dashv \Omega' \\\\
      \Delta \vdash S \subtype \_ \diamond v \dashv \Delta' \\\\
      \Delta : \Omega \\\\
      \oif{\owrite{\diamond}}{\Omega \vdash v : t}
    }{}
    }{
      \inferrule{
        \Delta' : \Omega' \\\\
        \oif{\oread{\diamond}}{\Omega' \vdash v: t}
      }{}
    }
  \right]
\] \end{theorem}

\begin{theorem}{Statement interpretation is monotonic} \[
  \text{for all} \diamond \text{in} \{ \concreadarrows \} . \left[
    \oimplies{
      \inferrule{
      \Omega \vdash S \subtype S' \movearrow v \\\\
      \Omega' \vdash S \subtype S' \movearrow v' \\\\
      \Omega' \subtype \Omega
    }{}
    }{
      \inferrule{
        \Omega' \vdash v \subtype v'
      }{}
    }
  \right]
\] \end{theorem}

\begin{theorem}{Expression interpretation is monotonic} \[
  \text{for all} \diamond \text{in} \{ \allarrows \} . \left[
    \oimplies{
      \inferrule{
        \Omega_0 \vdash M \diamond t \dashv \Omega_1 \\\\
        \Omega_0' \vdash M \diamond t' \dashv \Omega_1' \\\\
        \Omega_0' \subtype \Omega_0 \\\\
        \oif{\owrite{\diamond}}{\Omega_0' \vdash t' \subtype t}
      }{}
    }{
      \inferrule{
        \Omega_1' \subtype \Omega_1 \\\\
        \oif{\oread{\diamond}}{\Omega_1' \vdash t' \subtype t}
      }{}
    }
  \right]
\] \end{theorem}

\begin{theorem}{Statement checking is respected} \[
  \text{for all} \diamond \text{in} \{ \concreadarrows \} . \left[
    \oimplies{
      \inferrule{
        \Omega \vdash S \subtype S' \diamond t
      }{}
    }{
      \text{there exists a $v$ s.t.} \left[
        \inferrule{
          \Omega \vdash S \subtype \_ \diamond v \\\\
          \Omega \vdash S' \subtype \_ \erasedreadarrow t \\\\
          \Omega \vdash v \subtype t
        }{}
      \right]
    }
  \right]
\] \end{theorem}

\begin{theorem}{Subtyping is transitive} \[
  \oimplies{
    \inferrule{
      \Omega \vdash a \subtype b \\\\
      \Omega \vdash b \subtype c
    }{}
  }{
    \Omega \vdash a \subtype c
  }
\] \end{theorem}

% \begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Hole]
%   test
% \end{tcolorbox}
  
\section{Performance}
\label{section:performance}
\todo[inline]{maybe explicitly talk about cache coherency and pointer indirection and function pointers}
For the presented design to be good, it must describe a language with semantics that can easily be translated into efficient machine code by a compiler. This section discusses the various language features of Ochre and their performance implications.

Due to having a borrow checker, the abstract interpretation introduced in Section \ref{section:abstractinterpretation} statically determines when objects are dropped, which means it can insert any necessary memory frees into the resultant binary, removing the requirement for a garbage collector.

Being able to mutate data structures in place also allows programmers to express efficient algorithms provided they don't break the \textit{aliasing xor mutability} invariant, like Rust.

Ochre does not have native machine integers, which restricts the programmer to using Peano arithmetic or similar. This is disastrous for performance, and would not be tolerated in even the slowest languages. However, the design presented, and its type checker are perfectly compatible with integers (they would be similar to atoms), so while this work does not directly include efficient integers, I consider them compatible with it, and adding them would be a matter of engineering. The decision not to include them is discussed further in Section \ref{section:designdecisions}.

As presented, the type system does not support unboxed pairs, which means Ochre programs as currently stated have a lot of unnecessary indirection in their data structures. Much like not supporting machine integers, this is intolerable for a production systems language. However, the type system as presented is compatible with adding them in the future, and adding them at this point would detract from the core concepts. To demonstrate this compatibility, Appendix \ref{appendix:unbox} lays out a potential method for adding unboxed types.

Ochre does not have efficient contiguous arrays, which hurts the implementation of several dynamic structures, but after unboxed pairs are implemented, contiguous arrays are just many nested pairs. It is unclear how you would efficiently lookup the $n^{\text{th}}$ element in such a structure, but I am hopeful it would just be a matter of engineering/adding the right optimizations.

% \section{Ergonomics}
% \todo[inline]{implement}
% \todo[inline]{compare with ATS, Low*, and Aeneas, although this might be very tricky beyond toy examples}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \chapter{Ethical Issues}
% % Ethics checklist: https://wiki.imperial.ac.uk/display/docteaching/Ethics+Process
% I do not foresee any ethical issues arising from this project.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Bibliography
\bibliographystyle{plainnat}
\bibliography{references}

\appendix 
\addcontentsline{toc}{chapter}{APPENDICES}

\chapter{Appendicies}
\section{Formal Verification using (Dependent) Types}
\label{verificationwithtypes}

The primary motivation behind adding dependent types to a language is so you can perform theorem proving/formal verification in the type system. In some languages, like Lean, this is done to mechanize mathematical proofs to prevent errors and/or shorten the review process; in other languages, like F*, Idris or ATS this is done to allow the programmer to reason about the runtime properties of their programs. However, they are all just pure functional languages with dependent types, whether you choose to use this expressive power for maths or programs the underlying type system is the same.

So the question is how can you represent logical statements as (potentially dependent) types and use the type checker to prove them? This is best understood via a simpler version: proving logical tautologies using Haskell's type system.

\subsubsection{Boolean Tautologies in Haskell}
The Curry-Howard correspondence states there is an equivalence between the theory of computation, and logic. Specifically: types are analogous to statements, and terms (values) are analogous to proofs. Under this analogy, $5 : \mathbb{N}$ states that $5$ is a proof of $\mathbb{N}$.

We can use this to represent logical statements as types. Here is how various constructs in logic translate over to types (given in Haskell).

\begin{tabularx}{\textwidth}{ X|X|X }
  Logical Statement & Equivalent Haskell Type & Explanation \\
  \hline \\
  $\top$ & \verb|()| & Proving true is trivial, so unit type. \\
  $\bot$ & \verb|!| & There exists no proof of false, so empty type. \\
  $a \Rightarrow b$ & \verb|a -> b| & If you have a proof of $a$, you can use it to construct a proof of $b$. \\
  $a \wedge b$ & \verb|(a, b)| & A proof of $a$ and a proof of $b$ combined into one proof. \\
  $a \vee b$ & \verb|Either a b| & This proof was either constructed in the presence of a proof of $a$ or a proof of $b$.
\end{tabularx}

For example, to prove the logical statement $(a \wedge b) \Rightarrow a$, we must define a Haskell term with type \verb|(a, b) -> a|, which can be done as such:

\begin{lstlisting}
proof :: (a, b) -> a
proof (a, b) = a
\end{lstlisting}

For another example, we can prove $((a \wedge b) \vee (a \wedge c)) \Rightarrow (a \wedge (b \vee c))$, which you might want to convince yourself of separately before moving on, by providing a Haskell term of type \verb|Either (a, b) (a, c) -> (a, Either b c)|.

\begin{lstlisting}
proof' :: Either (a, b) (a, c) -> (a, Either b c)
proof' (Left (a, b)) = (a, Left b)
proof' (Right (a, c)) = (a, Right c)
\end{lstlisting}

% We now set out to represent statements in logic as types, and their proofs as terms.
% \begin{itemize}
%   \item A proof of $true$ should be trivial, just by knowing that we are trying to prove $true$ should be enough for us to construct the proof. For this reason, we choose to represent $true$ as the type with a single term, $\top$; otherwise known as $1$ or $()$ (``unit''). There is exactly 1 term of type $\top$, which is also denoted as whatever the type is. E.g. in Haskell \verb|(): ()|. This means that whenever we need to generate a proof of $true$, we can do so trivially because the proof is $()$.
%   \item A proof of $false$ should be impossible since $false$ can never be proven true, fso we represent it with an empty type, $\bot$; also known as $0$ or $!$. There are no terms of type $\bot$.
%   \item Logical implication, $a \rightarrow b$ means if we're given a proof of $a$, and can derive a proof of $b$, we have proven $a \rightarrow b$. It is as though the proof is taking another proof as input. We represent logical implication as the function type, also denoted by $\rightarrow$. For instance, if we could make a type-checking term \verb|f| of type \verb|() -> ()| in Haskell, we would have proven that $true \rightarrow true$. This is trivial with \verb|f () = ()|.
%   \item Conjunction, $a \wedge b$ (logical and), means we have proven both $a$ and $b$. We represent this with the pair $(a, b)$. To get a term of type $(a, b)$ we must have both a term of type $a$ and a term of type $b$, which is analogous to having a proof of $a$ \textit{and} a proof of $b$.
%   \item Disjunction, $a \vee b$ (logical or), means we have either proven $a$ or $b$. We represent this with the slightly less mathematical \verb|Either a b|. If we have a term of type $a$, we can produce an \verb|Either a b| with \verb|Left a|, and if we have a term of type $b$ we can produce an \verb|Either a b| with \verb|Right b|. This is analogous to having either a proof of $a$, \textit{or} a proof of $b$. 
% \end{itemize}

With this we can construct proofs for logical tautologies, but how do we go further and construct proofs for statements like ``If you get any number and double it, you get an even number''.

\subsubsection{Dependent Types are Quantifiers}
Let's now define a function $even$ which returns a type, such that any term of type $even(n)$ is proof that $n$ is even. To do this, $even$ returns a \textit{type}: $\top$ if $n$ is even, $\bot$ otherwise. I.e. $even(4) = \top$ and $even(5) = \bot$. The logical statement $\forall n : \mathbb{Z}. even(2n)$ can be represented by the type $(n: \mathbb{Z}) \rightarrow even(2 * n)$. If we had a term of this type, we could give it any integer $n$, and it would return proof that $2n$ is even.

This cannot be represented in Haskell, because $(\textbf{n}: \mathbb{Z}) \rightarrow even(2 * \textbf{n})$ is a dependent type, hence we need a dependently typed language like Agda. This is an example of Haskell's non-dependent type system not being able to express quantifiers like $\forall$ or $\exists$ over values.

% this is optional section
% types are statements, programs are proofs
% normal tautologies in Haskell
% how pi and sigma types denote qualifiers

% what is formal verification
% outline of various methods for doing it

\section{Supporting Unboxed Pairs}
\label{appendix:unbox}
While atoms, functions, and references are unboxed in Ochre, pairs are always heap-allocated. As discussed in Section \ref{section:performance}, this will hinder the performance of compiled Ochre programs. This appendix lays out a rough plan for adding unboxed types to the formal semantics for Ochre, to make the point that the research as presented is compatible with such an extension.

A potential method of adding unboxed pairs:

\begin{enumerate}
  \item \textbf{Add $\kw{box} \, t$, a new type which represents an explicit heap allocation}, along with corresponding constructors and eliminators. This is required so the programmer can heap allocate objects whose size is not comptime known.
  \item \textbf{Edit the abstract interpretations to pass around (type, size) pairs instead of just types}. This would involve all read arrows returning the size of the value being read, and all write arrows taking the size of the data being written.
  
  Set the size of pairs to the sum of the size of the elements in the pair. Because the type of the right-hand side can depend on the left, this can cause some sizes to be unknown. Because of this, arrows may return an unknown size, and if the user needs to put something of unknown size on the stack, they must put it in a box.

  The size of a match statement is the largest size of any of its branches.
\end{enumerate}

% With pairs being boxed, this isn't required, because every type is one machine word long (pairs are a pointer to a heap allocation of two values, and pointers are a single word). The definition for size on statements and values would look something like the 

% \begin{figure}[H]
%   \small
%   \centering
%   \begin{mathpar}
%     \begin{array}{c|c}
%       t & \odef{\kw{size}}{t} \\
%       \hline

%       \\(t, T \rightarrow S) &
%       \inferrule{
%         \Omega \vdash \size{t} = l \\\\
%         \Omega \vdash T \erasedwritearrow t \dashv \Omega' \\\\
%         \Omega' \vdash \size{S} = r
%       }{
%         \Omega \vdash \size{(t, T \rightarrow S)} = l + r
%       } \\

%       \\M &
%       \inferrule{
%         M \textit{is not a pair}
%       }{
%         \Omega \vdash \size{M} = 1
%       } \\

%     \end{array}

%     \begin{array}{c|c}
%       S & \odef{\kw{size}}{S} \\
%       \hline

%       \\M &
%       \inferrule{
%         \\
%       }{
%         \Omega \vdash 
%       } \\

%       \\M &
%       \inferrule{
%         M \textit{is not a pair}
%       }{
%         \size{M} = 1
%       } \\

%     \end{array}
%   \end{mathpar}
% \end{figure}

\end{document}
